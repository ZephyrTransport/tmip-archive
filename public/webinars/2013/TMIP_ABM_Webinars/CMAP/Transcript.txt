<meta name="viewport" content="width=device-width; initial-scale=1.0; "\>Event ID: 2081897
Event Started: 1/31/2013 7:00:00 PM
----------

 Please stand by for realtime captions.
>> Good afternoon. Welcome to today's TMIP webinar on activity freight modeling -- advanced freight modeling at the Chicago agency for planning. My name is Brian Grady and I serve as a support contractor for team outreach and I will be moderating. CMAP has been trying to address the inadequacies of the current models in freight demand forecasting that were originally developed for representing personal passenger travel. And you too were based and logistics and supply chain model for this a cargo Metropolitan area has recently been developed and tested. This webinar will cover these new model initiatives motivations and policy considerations. I have a few administrative items to mentioned before we begin. In order to provide the best possible audio experience we have automatically muted all participant phone lines. At this and other webinar we will be posting a brief questionnaire. We are interested in your feedback for how to improve this and future webinars. The poll results are one of the factors we used to determine our -- determine webinar format. Will be taking a 1 min. rake at the end before the Q&A section to allow you to fill out the evaluation pole. 
>> This webinar will last two hours and we will have one and a half hours of presentation followed by up to 30 min. of question-and-answer. You can submit questions via the webinar needing room software. A Q&A pod window is displayed on your screen and you can enter questions there any time. They will be posed to the presenters during the Q&A session at the end of the webinar. We are recording this webinar and will make it available on the TMIP committee of practice within two weeks. The webinar is also being live closed-captioned for the hearing impaired. If you do not wish to view the closed captions simply click the full screen button in the top right-hand corner to maximize the slides view only. 
>> And finally the information in this webinar is made available for knowledge and experience sharing purposes only and does not represent an endorsement by FHWA. 
>> I'm pleased to introduce today speakers. Kermit Wies is the Chicago Metropolitan agency for planning where he serves as deputy executive director for research and analysis. Kermit has over 27 years experience in urban systems modeling and planning and is the principal author of the 2030 regional transportation plan to the Chicago Metropolitan area. And over the past several years Kermit has been overseeing CMAP development and dictation of techniques including agent-based economic application for freight. Monique Urban is a travel demand forecast or at Cambridge systematics. In Chicago where she participates in a broad range of modeling studies. She has developed models of their choice and ridership for regional transit agencies. Currently development models of utilitarian and recreational trips for bicycle and is equally interested in decision-making and [ Indiscernible ]. Marion out water is the director of freight forecast event -- method that our SG&A published in a wide variety of advanced modeling topics. She manage the research and development of the freight forecasting framework based on logistics supply chain and tour based methods for MPOs in the United States sponsored by the federal Highway administration and is currently being able blinded by the Florida Department of Transportation. Today's presenters are acknowledging their work which will be presented today was to a collaborative effort and experienced team of individuals representing many different agencies firms and institutions made important contributions to this project and they are identified here. With that Kermit, why don't you take us away. 
>> Thank you. I would like to begin by giving a couple of contact points for as -- for CMAP. We have a group audit -- that has a home on our website or agency if you put a/At the end of that you will see a lot of useful information on that comes out of our user group. I personally am happy to discuss our modeling work off-line so if you want to contact me by e-mail you can use that same CMAP Illinois.gov and put AK Deb QIES at the front of it and I will be sure to get it.
>> I would like to begin by giving a little bit of background about how CMAP came to be interested in doing advanced freight model development. Freight -- as you know has always been the ugly stepsister of MPO models. We try to shoehorn musty old surveys were badly mangled administrative data into the glass slipper of our four-star procedures that we used for personal travel. And the reason we do this is it makes for a quick way to layer trip tables needed to get through traffic assignment. For many years that was all necessary and this is the approach that has been elaborated extensively by FHWA in there. Response freight manual. It offers a lot of tools to do -- use this four step approach at least to the best of its ability. And if the goal is to mimic existing track count so you can get Correct Available Hwy., Simon or omissions estimates and that's probably fine but if the goal is to explain the public policy or infrastructure in investments practices of businesses that are engage in the grids movement than the static approach is inadequate. So what CMAP did as a first step was we included a lot of fairly clear guidance in our new long-range plan. In 2010 we produced our first long-range plan that was patterned after the original burn a model where instead of being a regional transportation plan we address the multiple facets of urban planning in one policy. The entire plan was called go to 48. Within that copper has a plan visit free chapter. It addresses more than just the freight related infrastructure and congestion management outcomes in a traditional RTP. The first one -- the first thing we do is join a rising chorus of states and regions to develop national policy. Any abashed search for data knows there is precious little that comes from the policy side and so even for modeling this national policies and important point because such a policy would by definition be accompanied by some standardization of data resources that are used for making freight investment decisions. And those data resources would inevitably become the building blocks of freight model application. Go to 2040 our regional plan highlights the obvious connection between freight and economic development. This is obvious in the regional like Chicago because we are a freight hub but it is not really an important area for public policy were planning because most of freight operations are positively held. But there have been collaborations and partnerships that have resulted in investing public money on behalf of privately owned infrastructure and this is been loosely justified in terms of a general economic welfare benefit. Sorted such as more jobs or greater commerce in the region. None of that has been rigorously model. Finally we need to prioritize or in some cases differ along with capital maintenance needs for our infrastructure and like the economic development criteria our long-range plan calls for a higher standard across benefit accounting when selecting projects. So needless to say the traditional freight forecasting approach doesn't withstand scrutiny when challenged on these planning frontiers so in the plan itself it calls upon CMAP to develop stronger analytical tools for evaluating freight costs and benefits. To kick things off we needed a guiding framework. We had Hardy developed a strategic plan for advanced modeling that was intended to guide us into -- activity-based modeling practice but activity-based modeling is focusing on person travel and replacing the fourth step. So the plan didn't say much about how freight is fundamentally different and left as close to trading freight as ugly stepsister and taking with the activity-based model. But just a little bit of searching turned up a proposal that had been developed for Los Angeles by originally by Mike Fisher and Maren Outwater. To evaluate choices and supply chain logistics the network operations associated with freight. And even though the proposal wasn't implemented in LA it was implicit in the abstract I uncovered that coupling these three elements would be accomplished there, data definitions and the treatment of those throughout. 
>> Finally what we needed was an implementation strategy. The strategic plan called for new models developed by CMAP to be aged based micro simulations. First it was consistent with the standards established by CMAP that this is what we were going to put on modeling money now that the models were theoretically more robust and computational. The second and maybe even more important was that microsimulation the behavior of individual agents offers more intuitively satisfying defense of the model output. The model is no longer a metaphor for the scenarios you are describing. Is the actual story of the scenario. And with regard to freight this might be most obvious in the literature you can find around logistics and supply chain modeling and analysis -- most of his litter -- literature originated in the realm of MBA programs and management programs and they are devoted not to forecasting but to explain the cumulative singles and actions and effects of discrete business races in the classroom. So they are teaching tools and not used for forecasting. This is a frontier without a doubt. Today's topic is the development of CMAP mesoscale phase. This is gone through two rounds of development. One by CSN RSG. And then I will talk about how the data resources for the -- or the lack helped or hindered the model. I will talk about how to get started using the model. And then give you some examples of sensitivity analysis of scenarios and I will wrap up with how we go about using the application. After I am done this will take about 3 min. Monique Urban and Maren Outwater will describe more in depth the model development exercises. 
>> As I mentioned the genesis of this analysis was 2005 proposal that was offered by Cambridge systematics in Los Angeles County. And even though they didn't proceed with implementation the proposal was published at the 2005 TRB me annual meeting. In 2009 RSG proposed to implement the logistics and supply change component in Chicago under an application as the AA to FHWA and at the same time CMAP independently issued a development offering that each of the three tiers my be addressed separately or jointly on behalf of CMAP. Upon receiving the proposal CMAP awarded the contract to see as per their proposal to develop the mesoscale freight model. At the same time FHWA awarded the request to RSG. While the proposals overlap they were not identical and the two teams worked quite well to coordinate their efforts avoiding duplication and building on each other's contributions. But he could will present later was the deputy project manager for the CMAP work and Merrin was the project manager for the FHWA work. 
>> Let's start with the data resources. I put parenthetically because that will become a regular topic of discussion with regard to doing freight modeling. We're in a crisis with regard to availability of data and almost areas of travel demand. But in particular in traditional surveys that become unsustainable from both a cost and accountability standpoint. There are privacy concerns and standards for accountability have rendered the legacy for travel tires -- diaries absolutely. This is up to the data program as the survey accounting business pattern and the vehicle inventory and use survey. But needless to say necessity is the mother invention and the need to make the most of out dated resources as well is occasionally find the necessary to outright fabricate intuitively possible but not provable relationships lend itself well to the agent-based paradigm because rather than using representative sample to calibrate a snapshot of universal phenomenon available data can be used to sequence rational choices by individual agents that when combined render an equally convincing likeness of the outcome. Be prepared in this discussion and subsequent presentations to see these resources that are showing on this slide used in unconventional ways. Nowhere more obvious than in the estimation of coefficients. We did use some traditional estimation techniques for DataViz -- the need to Park a [ Indiscernible ] was permissible and encouraged by CMAP. 
>> So let me jump ahead to the application itself. Remembering some of those caveats about the data I want to address potential user approach so if you're tuned into the webinar because you think you might want to get your hands on this model and use it pay attention. This is not a commercial software application and there is no provision for user support aside from contracted with one of the developers. The team that put this together are in a delicate critical phase of building a community of users that have the capacity not only to properly learned this application but contribute to its improvement. This is in a plug-and-play application and if you do decide to take it on be prepared to wait -- way into the mud with the rest of us in particular the expectation you will document and share both the good and the bad and the ugly that you encounter. What you see appear on screen is our internal key we use among our staff here to keep track of what we learned. This environment is really good rather than just firing questions through e-mail to keep track of the progress. One thing you'll have to do is learn art. And learn how to interpret the RS statistical program application. CMAP is a long time statistical analysis shot so CS originally coded the application to facilitate CMAP spec use. Staff is a proprietary statistical package and the user licenses costly. It was FHWA's goal but the cost not be a barrier to transferring this code UEDA region so RSG scope of gloated -- included writing the code in ARC. We have -- we have CMAP people becoming our users. Three and Summers to the users. We have an open-source program and it's managed improved by a community of users and even though the code is tightly controlled the accountability for performances and Terry the same weight as a licensed a proprietary product. So you should expect that the program that work yesterday is going to work today because they did something on the [ Indiscernible ] side that introduced a bug. Second thing is understand the logic. While the consultant will -- conceptual parallels to force that modeling -- we use a firm -- expression firm generation and way similar to trip generation. This parallel will help you understand the new framework. The Internet -- internal logic of the code is different from anything you've seen. Study the code annotation using newly syntax skills to understand what's going on. 
>> The third is to scrutinize the data. If you have some experience using activity-based models understanding the data structures will be easier to comprehend. Most of the data takes the form of activity records associated with the transaction along the supply chain. It is disaggregate and nearly inscrutable in raw form. It does offer almost endless capacity to isolate problem data points and queries summarize and translate into other formats such as trip tables. Next understand the output. Remember that this is -- this model is micro-simulating commodity flows so this is different from a truck trip generation model. Our estimate is that the model accounts for less than 50% of the commercial vehicle traffic on the road. The rest being made up on service calls and parcel delivery and you would validate if you are trying to validate a highway assignment. What this does with that 50% however is more theoretically robust. 
>> A little bit about tests we run. We have not used this application for demonstrating conformity or project studies but we have a couple policies scenarios we've looked at and in implementing our long-range plan. One is to reduce rail delays in Chicago. Chicago is a rail hub and we introduce a lot of congestion related delay to cross continental rail freight movements. Some of the solutions to that are operational but others are to add rail capacity. This is -- this picture is a cool YouTube video that shows how they installed a new bridge. The cost to benefit for this project were was not a ball awaited but when you watch the video you will begin to imagine what the expense was encountered and it was again part of this partnership for public money replacing this rail bridge. You get an idea of why we want to use these tools to decide whether project is worth it. First area was to reduce rail delays. The two circles you see one gives a real-world mapping application of the very strategic operational signal improvement and crossovers and less capital incentive improvements. The models presentation is on the right in the network model. This map doesn't [ Indiscernible ] a lot of things are done with sticks and balls but basically you code these into the network in much the same way you would with a traditional network model and run. 
>> That central nerve we cut the logistics times of the Chicago central yards by half. We said it was going to happen. What happened the model turns around a new mode share between rail and truck and it also -- says this is a commodity for model will give you the enumerated output I commodity classification. In doing this we saw a greater amount of motorized vehicle -- this is tracks the move between the yard so much more dredge and vehicles and much less articles of base metal. 
>> It does give you raw tons of shipments that are moving. The stops on the left column are logistics know in the model network and we get a summary of the tons annual that are moving through those logistics modes. 
>> We added rail capacity between intermodal cut yard and a court. Thinking that would be a sensible thing to do. And we get the same results. Increase in rail and drop in trucks and a drop in truck dredge and changes in terms of shipments. These are the things when doing a cross benefit analysis intended to address the economic benefits of the freight cluster in Chicago. People want to see this. Finally using the application install arm you go to the project.org and downloading is straightforward and simple. If you have IT firewalls them I need help from your IT department but I did this at home and I did it at work there was no problem. 
>> Inspect the model code. RSG MCS did a good job and it is now organized in a way that even is quite intuitively understandable by reading the names of the files. When you open any one of these files you will see a heavily annotated script that will care you through the logic. There are places where you have to hard code changes. So you do need to inspect the code and it won't run right out of the box. Become familiar with your input. Most of the model inputs are CSV files that can easily be read in Excel and independent scrutiny of the model inputs in the quality is a good professional practice and the model will run with any kind of garbage for in so if you're not familiar with what is in the input you will run into trouble. And it will be the models fall. Finally the fun part let 'er rip. You type -- which you have the input the code in place you type the executable file into the GUI and you're off and running. The Chicago region takes 90 min. to run so it's a good opportunity to write in your wiki about the fun you are having. 
>> Inspect the output. This will take practice because if you're used to working in a conventional travel demand software you are missing a lot of the detail that this proposal will give you. The pivot tables were all products of Microsoft Excel so these files can be dumped into Excel if you know how to use the pivot table features and you're off and running to create wonderful summaries of what the model scenarios are telling you. 
>> That was it for me. 
>> Thank you Kermit. 
>> I will be talking about -- I'm Monique Urban and I'm not sure where my title slide went. 
>> Mine disappeared also. Just wing it. 
>> Today I will be talking to you about supply chain logistics models that we helped CMAP to develop. My colleagues on this project were Dan begin and Mike Fisher and also of CS along with [ Indiscernible ]. And of course we would like to thank Kermit and the other CMAP staff for their contributions on this. 
>> Just to give a quick overview of the material. First I will talk about the objective of the studies. Increment description. Almost mainly describe how we interpreted the objectives for the models. I will talk about the history of freight modeling and how this model fits into that evolution. Then I will go through the steps of the model and example results of that the model can generate. 
>> First as Kermit mentioned CMAP wanted to develop this model because it was getting a lot of questions that could be answered by its conventional freight modeling tools. These are questions that some of you in the audience they also have these questions or similar questions for your region. Questions like how is mode share impacted by macro economic factors such as fuel prices. Or if the region were to build a new airport how could that relieve congestion at existing airports. If there were a new intermodal terminal or other facility like that would help to reduce truck trips within the region. If the region were to invest in truck only lanes with this help to draw tracks off of existing facilities and how efficiently would that be utilized. Think of a context to how we chose to answer these questions and CMAP gave us a lot of latitude in designing the model. We gave a great consideration and thought about everything from the most basic kind of model which would be something like factoring automobile trips or doing for messes like estimating trust [ Indiscernible ] rates all the way up to the disaggregating commodity flows or perhaps developing a torn model. We settled on a supply chain and logistics model as being the most appropriate form to help answer some of these economically driven questions that CMAP was trying to answer. 
>> Here Kermit did a good job summarizing the three-tiered framework that CMAP was proposing. I will go through this briefly. We have the three-tiered modeling framework with the macro scale generating some high-level commodity flows. A global level. And -- the mesoscale model which takes macro scale flows and figures out who in the Chicago region is trading these flows. And what decisions do they make that relate to the shipping of these goods. 
>> And finally some day there will be a microscale model that will be used to simulate [ Indiscernible ]. 
>> This shows some recent development and advanced freight modeling especially the supply chain and logistics area. Back in the 1990s there was -- a couple notable efforts. The good trip of the smile models developed a broad that belong in the genre. There was three-tiered modeling framework developed by LA Metro and CS in 2005 and that became the guiding vision to this project. In 2007 and ongoing I believe the aggregate disaggregate aggregate model application developed for Norway and Sweden. Then there was the freight activity microsimulation estimator developed in 2010 by researchers at University of Illinois Chicago. Who we partnered with on this project. In 2011 we completed our work and of course we shared our code with the FHWA team to become part of their effort. So some of the parameters to this project. We had some leeway in how we were going to design this model. [ Indiscernible ] parameters that they wanted us to follow. Kermit did a great job outlining some of these. This is going to be an agent based model driven by business economic principles. We had to use a software that was transparent to CMAP and could be readily adapted and understood by them. So we chose to use [ Indiscernible ] for this. We developed a network in and lead to a multiple nodal -- a multimodal network. 
>> Then in order to make the project meaningful for analysis of the Chicago region we created these mesoscale zones which are roughly 3 miles x 3 miles within the CMAP region. Outside of the CMAP region be simply use the FAS or foreign zones we used FAS which are broad aggregations. 
>> We developed a multi-no modal -- multimodal network. With information from Oak Ridge national labs and Army Corps of Engineers. The rail network is national. At the national level I should say. It includes the class I railroads and at the regional level and also includes the class once and a couple of example regional lines. We included a water network meaning to model greatly traffic. And then we included the two main airports in the region. Midway and O'Hare. And then a couple of others. We couldn't envision doing a model of this scale without making -- that covering all of the modes that are commonly used for freight today. Including rail card road and intermodal carload in model and air. A truck mode where the container is called directly from the power to the destination. Full truckload and less than truckload. He we included logistics handling facilities in the network to represent the broad variety of logistics handling that occurs at these important points today. Intermodal yards and distribution and that translation. 
>> Now I will give a step-by-step overview of what the mesoscale model does and then I will talk more about each of the steps. And you will note there are a lot of parallels activity-based modeling throughout this framework. Probably most notably in the firm synthesis which is much like population synthesis which is step one of our model. Instead to the firm care together to form a supply chain. And step three the high-level commodity flows from the macro scale model are then a prescient among the supply chain. And step or each subtype chain selects a path for shipping goods from the spire to the supplier to the buyer. And then in step five because this is a travel demand model after all we prepare the results for assignment. 
>> First there is a step zero which is implicit to all this which is preparing the macro scale data. So macro scale model is not yet developed so for now when we developed this we decided to use the FAF commodity flow of data. This data set is based on the commodity flows survey which if you are not familiar with it includes the origins and destinations of commodity shipments and it samples firms from several -- all the major back -- factors involved in shipping goods. This includes the manufacturing mining and wholesale factors. And then select retail and service establishments are included in that. And a quick note on why we chose the [ Indiscernible ] data set over the transfer data set. Which is also available for use and the reason I mention this is this question often comes up. The FAF data set flows unbroken flows from the production to the consumption and and there may be a wholesale trade in their but the transfers data focuses more on providing data set that has the entire trip already broken up so you know you will see the last leg of the trip as it enters the region on trucks for example. But we wanted to use the FAF data said because it contained the full past and the power of this model is that it can -- it breaks down the path for you so this provides -- this provides the capability we want. We want to know -- be able to model the goods are stopping. Where they're getting handled and whether -- we use that new intermodal yard. This data set because of the sample with these types of businesses we included suppliers and buyers and wholesale firms where suppliers are clearly producing some kind of good and buyers are clearly consuming some kind of good. And most firms do both. And then wholesale firms which are definitely both shipping and receiving some goods. In step one the firm synthesis step we start with a [ Indiscernible ] pattern data and then supplement it with placeholder data in the area where the CBP data are lacking. Mainly agricultural and construction and then we put in some placeholder data for foreign firms. This is pretty similar to what was done in the same model. We characterize firms as buyers, suppliers or both. We identify the top commodities that are traded by each firm based on information from the make and use tables. And then for wholesale firms we simulate the types of goods that they are trading. So in this way wholesale firms which were basically represented in the FAF data are represented in the modeling Primark and the two pieces are consistent. The putting on how the macro scale data ultimately are developed this could change. 
>> For the next step -- we have now accounting level data set of businesses. The number of businesses by size and by industry categories. But to make it meaningful for the Chicago region we wanted to -- we needed a way to better identify where these firms are located. So to do this we developed a firm location model and using land-use data from CMAP we simulate a specific location for each of these businesses. 
>> In step two the supplier selection stage we first identify a wide set of potential trading partners for each buyer firm. This is similar to what was done in the same framework as well. Here we utilize information from the input output make and use tables for the Bureau of economic analysis and we put additional constraint that candidate training partners must be part of the macro scale commodity flow table. Just keeping it consistent with will go -- with going on at the macro scale level. Each fire selects a supplier to form what we call a buyer supplier pair or a supply chain. This election process now is based on the same formulation which relates the business size of the consumer and producer and it uses a parameter for the distance between the two businesses. In step three they commodity flows from the macro scale level are at apportioned among the individual supply chains -- the supplier buyer pair. This is based on two things. We use of the buyer firm size i.e. the number of employees as well as the tons of goods consumed by each fire employee. This is a factor that we created using information from that make use table. And it specific to industry. So a lot of times you hear people say you can't really disaggregate based just on the number of them please because that is not necessarily related to the amount of output or commodity consumed and that is right. That is why we created this factor. The output this generate the number of annual times that are traded between each hire and fire. Next we go through the path selection stage and tier this uses a path information for the model network. There are 54 different paths that include different modes and sub modes like [ Indiscernible ] versus full truckload. Whether or not a logistics handling facility was used and if so which one was used. And to help with -- to help identify we used a formulation of annual transport and logistics costs. The same formulation developed by [ Indiscernible ] and [ Indiscernible ] for the ADA framework. I should mention that this formulation is really based on well over 50 years of research and operations research and other areas. So it's quite comprehensive. We like it because it includes the whole gamut of factors that are considered by shippers today. Trade-offs between inventory versus shipping and pipeline inventory versus things like damage. And of course the more obvious things like secure cost by mode. And the reliability. 
>> This is very -- varied by commodity type in order to come up with the full set of paths. So at the end of the day each supply chain has selected a transportation -- transport image. At this point as the model gets ready to compare the data for assignment you know how this huge table with just a wealth of information and to Kermit describe some of this. The basically if a table that has for each buyer and supplier pair it has a commodity being shipped and the size of the shipment and the frequency and shipment. The mode that was used. And that has the origin and the death -- destination and in every intermediate stop loads. This data can then be aggregated and any number of ways. For assignment obviously you would aggregate by this origin and destination TAC and account for things like how many tons in a truck and stuff like that. So at this point I will go through -- is a lot of information to present in words so now I will go through an example that shows more graphically how the model works. Supply chain example looking at consumer goods that are being shipped from overseas manufacturers or let's say and East Asia for this example to retailers in the Chicago area. And as a mentioned before our input flows are derived from the macro scale model although for now we are using tons. 
>> So in step one regenerate firms and manufacturing firms and the zone and retail firms in the Chicago area. Instead to we've form supply chains for each of buyer to pick the supplier to trade with. In step three they commodity flows are apportioned among the supply chains. So it is determined how many tons are being exchanged between each buyer supplier pair. And then in step four the past selection process which is some ways is the most compensated part of this process and I'll spend the next couple of slides talking about this. Here -- for this example is focused on the portion where it is traveling from the court to the Chicago region. 
>> There is a number -- a number of options for the path and the modeling numerate these options. There are some 54 options. Once it arrived in the port be translated from 20 foot containers in 253 good tracks and the goods could be brought to a rail terminal and brought by intermodal rail to the Chicago area. And then distributed in the Chicago area. The goods could go through some warehousing or distribution in the port area and then brought by rail or possibly talked to the Chicago area. So any number of options here. And that is called the packing enumeration process. 
>> Let's consider a shipment of 140 time traveling in -- arriving at the Port of LA in 740 foot containers. With Option A these seven tracks could haul the seven containers for the entire distance. At a certain cost. In Option B the goods first did transfer loaded into 50 53 foot tracks to make five tracks instead of seven tracks to have some cost savings on the long portion and then Option C the goods are popped on and -- popped onto it intermodal rail and brought to Chicago and distributed and to the final destination by truck. To compare these options the model formulation goes through and it calculates the transportation costs associated with each of these which included the linehaul portion as well as the cost of any logistics handling. And that it accounts for other factors like potential delays or chance of loss and damage. And then each supply chain selects a single pass for shipping goods. 
>> The model gives a huge wealth of data that can be summarized in a number of ways to present interesting information. I will show you a few examples results here. Of things that you may find interesting for your region. So here we can look at -- summarize the number of shipments or the total tonnage is of goods that are traveling by each of the modes. Or by any other characteristic of the path type. Here refocus on the main mode like air and directs tracking and intermodal and full truckload for the long haul portion and less on track distribution. Carload and water. We group the commodities into these three classes. Finished goods which pretty something like retail goods. Intermediate process goods or maybe chemicals or something like plastic. And broke or natural resources which include coal and sand and gravel and food grains and large shipments like that. What you would expect is that -- and what we see that the model is that most of the bulk shipments are choosing the modes you would expect for example carload or truck mode and water. Finished goods in contrast really dominate in the air. They also used truck load and some intermodal but not so much carload although a little share there. Intermediate process goods is somewhere in the middle for most of these. 
>> And then in this table I do want to go through this table piece by piece but the information in this table with -- was summarized from example model output and the information here is used to present the develop the next two slides that I will be showing. This shows the number of annual shipments and the number of drainage tracks generated at the various facilities in the region. Truck terminals at airports and water ports an internal yards. Putting these into a map format and truck terminals aren't shown because to keep it simple. But focusing on the air rail air and water terminals and focusing on the number of shipments you would expect to see a lot of shipments going through the airports. For lots of letters in small packages. That is what we see and we don't see a whole lot going through the other types of ports. 
>> For less than truckload drayage trucks we still expect to see a fair number of these tracks being generated at the airport but here you would expect to see a pickup in activity at the railyard. And that is what we're seeing here. We're seeing very little activity at these two water ports. I should've given you the land here but these are the two water ports. The Illinois international part of the Indiana Harbor -- the airport Milwaukee airport and O'Hare and Midway and Gary and these other ones are rail yards. 
>> Looking then add full truckload drayage trucks you would expect to see probably most of these at the railyard and then you would also expect to see some water ports because they don't really generate other types of tracks as much. And that is what we are seeing and hardly -- zero if any trucks of these full truckload trucks at the airport. In summary this is a pre--- innovative model and it has a lot of parallels to a cavity -- activity-based models. Using agent-based approach to modeling freight movement. It is guided by economic principles and generates insights into a lot of questions that can't be answered by traditional freight modeling practices. This is a working demonstration model as current mentioned. Sometimes they weren't available data so an obvious next up would be to collect data and do additional calibration and validation.
>> Thank you everyone. 
>> My name is Maren Outwater and I'm with RSG and we as Kermit mentioned did a parallel study with trying to take advantage of the work that came -- cambric -- Cambridge systematics did for a demonstration for Chicago. But we also because this was a federally funded research project had another goal which was to develop a framework that could be adopted by Metropolitan planning organizations in the U.S. for use in evaluating transportation investments in their impacts on freight mobility. So this was a much broader goal in terms of trying to find something that would work in other places. As well as to demonstrate that it could work in Chicago. The motivation for this project really from the federal standpoint was current freight forecasting methods don't really address the complexity of freight demand. And that attempts to apply a similar approach to passenger forecasting hasn't worked that well since it is really quite a different animal altogether. And given the new techniques that were being explored both on the freight side and on the passenger side that these techniques should really be considered more readily for freight core -- forecasting including disaggregate approach to -- and eight are-based approach including econometric relationships and representing supply chains more directly. That was the initial project goal. As part of that we also looked at a number of the current weaknesses identified in standard practice. To make sure that we were addressing that. In terms of the framework that we have. The first one was the general lack of detail at the traffic analysis zone level. A lot of current freight forecasting models operate at a larger scale like a fast zone network level which is quite broad. And really not as useful. So we specifically oriented around synthesizing firms and movement at the zone level. Another issue is the lack of information about local pickup and delivery trips. At the regional scale this is a delivery system that most -- a lot of freight forecasting models don't really represent adequately. They are just looking at the long haul movement and not so much at the pickup and delivery system which constitutes a lot of truck traffic in any regional area. So we specifically looked at modeling the delivery system at the end of the supply chain. The third area was this interest in developing a relationship between the long haul and short haul forecasting movements in terms of demand. And the impact from the regional investments. So we looked at a single framework that could handle both the long haul and the short-haul movements. Before the area was the ability to capture trip training -- training and in this case it applies for long-haul movements in the supply chain which we identify as having different distribution channels. And also the short-haul pickup and delivery system that looks like truck during making deliveries inside the region. 
>> The last area was the need to represent commodities that are produced in -- and consumed. The reason this is so important is because the behaviors are quite different between different commodities in terms of which buyers and suppliers are linked as well as what the mode and pop relationships are and the pickup and delivery systems are also different. These are the main aspects of the developing the framework. And we set about also being good research project in looking at what else was out there that we could decide on using those elements of existing modeling systems that might make sense for this forecasting firmer. The first type of advanced forecasting model was supply chain models which tended to be national in scope and focus on modeling commodity flows. But they did introduce useful components such as firm synthesis and mode impact selection much like what Monique talked about in terms of Cambridge systematics work and Chicago. The second type of model we looked at were too were based models which tended to be more regional in scope and focus on modeling truck movements specifically they were not multimodal and they included components such as third-generation and vehicle type models and time of day models. As well as identifying all of the destination or pickup and delivery stops that were necessary to complete truck to her. The third area we looked at were really hybrid models and that were a couple of places we found that did combine the concepts of supply chain in tour based models. This was done primarily at the statewide or regional level but tended to have a national component representing the supply chain at the national level. And this was the bulk of the framework that we went forward with combining these two types of models. This flowchart shows the forecasting framework that we developed that could be used by other MPOs. And specifically the demonstration in Chicago. There is really two levels to this framework on the left-hand side of the screen is the national scale models and I will talk about each one of these in the course of the rest of the discussion so I won't identify them specifically here. They generally follow a similar framework to the Cambridge systematics work. Up through the developing production mode and intermodal transfer please. With one exception and that is we introduced distribution channel model which identified the elements of the supply chain. The other important piece of this framework is the regional scale model which are integrated on the truck flow side. So that we capture the dynamics of the long-haul movements with the short-haul pickup and delivery system inside the Chicago region. Those regional models look at vehicle and to her patterns as well as how many tourists per day are being made and the number of stops and the sequence of the stops and duration as well as the timing. To add to what both Kermit and Monique said there is a tremendous amount of data at the end of this process because this is all simulated first individually at the firm level and then individually at the truck level. So there is a lot you can do with that at the end of the day. 
>> I am going to start with an overview of what the national supply chain models look like. The very beginning is a firm synthesis process. This was originally developed at the University of Illinois Chicago. And incorporated into the work that Cambridge systematics did. We adapted that. The shares just the firms inside the Chicago region but firms are synthesized for the entire U.S. With different industries and different firm size categories. And the spatial resolution is defined by the zones within the Chicago region and that the larger past zones outside of the region. 
>> Once we identify the firms we choose supplier for each of the buyers. This is similar to what Monique talked about. This is part of the model we were able to build upon. And once we developed the buyer supplier pairs we can look at the distance -- distributions of these to see how they compare. This graphic target areas where there are peaks in the distance distribution and which would make sense from the perspective of initially the short haul movement are within the Chicago region and then add 6 to 800 miles. There is a natural relationship between Chicago and the East Coast as well as Chicago and Texas being too large areas where the trucks go. There is a peak which is approximately the distance between Chicago and LA or other parts of the left coast. I want to mention also at this time the research work that we did was very detailed in terms of the elements of the framework because we had some data limitations and some resource limitations. We chose to focus on to commodities. The first would be food products and the second manufactured goods. Throughout the rest of this presentation you'll see a lot of reference to the food and manufactured goods elements of the framework. The framework does work for all other commodities but the models were specifically estimated four these two. The distribution channel model is being one element of a new model that we introduced and originally we wanted to do a more complex representation of supply chains and that was pioneered by some researchers in Tokyo using Tokyo survey data which is quite expensive. They were able to model the supply chain of about whether it included retail or wholesale or producer and how many elements of those were included in each supply chain. We had more limited data available to us and so we limited this distribution channel model to the number -- the number of intermediate facilities that were necessary to get the goods from the producer to the consumer. In this case you will see graphics showing for a food shipments the number of -- the amount of commodities that is going directly from the producer to consumer versus ones that have at least one stop as either consolidation center or a warehouse or a distribution center. And it's very hard to see because it is so small. There are some with two stops and in the case of manufactured goods even three stops. That was the extent of the distribution channel. The four elements were the four categories that we identified looking primarily at just the complexity of the supply chain.
>> The next element in the forecasting framework was the shipment size and frequency models. These we added in order to identify shipment size and frequency over the course of a year. So we have an annual delivery frequency both of these are important elements of the downstream models and so they determine whether the shipment size is basically small or medium or large. And then how often those goods need to be delivered over the course of the year. Between each buyer supplier. Pair. 
>> The mode and transfer models really identify the work that Cambridge systematics did which was originally based on the approach pioneered in Norway and Sweden. These show some results of mode split both within the region as well as entering and leaving the Chicago region. And I won't go into a lot of detail here because we were able to build upon the work that was previously done. Well described by money. And then just to give an example and make it a little bit more easily understood. We have in this example a seller in Arizona that is in the petroleum and coal products manufacturing industry. And if buyer in Chicago in the food manufacturing industry. We assign the demand of 464 times of plastics and rubber. This could easily be the packaging needed for various kinds of food in Chicago. Then we identified the distribution channel and in this case there is one transfer facility between Arizona and Chicago and be to make the assumption that transfer facility would most likely be in the Chicago region so we identified the location there. Then we look at the shipment size. This would be relatively small and the simulate the actual size in this case 5500 pounds. And estimate the frequency. So 167 days per year which is about every other day over the course of the year. And then our -- we are able to estimate the probability that this shipment would be on one of the four modes and in this case it turned out to be truck. All the way through. So I would like to shift gears a little and talk about the regional tour based models which were integrated with the national -- we understand the long haul movement. We then look at the regional pickup and delivery system of those goods in the Chicago region. So the first thing we do is to make a conversion because all of the national scale models operate on an annual commodity flow basis and most regional travel demand forecasting models operate on a daily basis. So we need to convert the daily shipments and identify the specific location of the warehouse that you see here. So that is the first element that we do. 
>> The next model component and the next step of the process is to select a vehicle type and a tour pattern. We identified basically two different kinds of were patterns. One kind we call peddling and that means there would be multiple pickup and delivery stops along the way and the other one is direct meaning it goes like from the warehouse directly to the consumer. And we also identify three pipes. We have for tire vehicles and 6 to 8 tire vehicles and semi trailers. And this graphic shows some of the results from that. 
>> The next step in the process is to allocate shipments to tumors and stops. So we start with the shipments for single two were patterns and then we look at larger shipments and we identified that. They are much more likely to have multiple two were patterns with really larger shipments. The regional model sequence then for these shipments from a single warehouse after we know the type of vehicle type in the TrueType is that they get delivered to the series of stops that you see here on the map and related next part of the system is to identify the clustering of these stops in order to identify which track will pick up and deliver from which stops. And in what order. So part of that is to figure out how many tumors is necessary to deliver that a full load other the course of the day. In this case you will see a 12 were delivery system attitude to were delivery system. This is done on a multilevel legit model that identified how many tumors are needed. One of the other important part about the number of tours is that there is of course limitations as to how many hours a track is allowed to deliver over the course of the day. It is not 24. So we have an upper limit on how many hours a day any individual track is allowed to do pickup and delivery. 
>> So then we cluster the stops together in this example it is to tourist systems and clustering of these stops around each other to identify really the truck would try to optimize their pickup and delivery system. In the stop sequencing to identify which stop happens first in which one happens next we have something called the greedy algorithm which really is relatively simple and also relatively realistic because it looks at the next closest stop relative to where you currently are. And keeps on going. And so it shows in this manner how the stops my be sequenced in a way that all the stops into this particular tour. 
>> So the stop sequencing as you see here is also applied to the 22 were delivery system and the first cluster and the second cluster. In a way that is also using the algorithm and take the closest stop to the warehouse and put these in from there. 
>> The next element of the modeling system are stop duration models. These are multi-no real models that identify the length -- the length of time that is needed at any particular stop and they do tend to be quite different for direct deliveries versus peddling deliveries because of the size of the shipment that is being unloaded at any particular stop. Which tends to be much larger on a direct delivery. And that is why it takes longer to unload them. So this graphic would be very intuitive about what is needed in terms of loading and unloading. 
>> This -- the stop duration at every single stop allows us to create a bit of a time sequence so once we know how long the stops are at each place along the two were we also identify then what time the truck leaves the warehouse at the very beginning of the day -- what we've been assume it's the beginning of the day. This allows us to predict the time that each truck will arrive at a stop and then depart from that stop and arrived the next up. All the way around. This shows the thing a little after 5:00 and arriving back a little after four clock. 
>> In the last 2 min. I wanted to say a few words about the demonstration we did in Chicago and also about another project where we are implementing this framework in another state and that would be Florida. So the demonstration in Chicago both Kerman and Monique covered this quite well so I just went to say a few things about it that we did estimate new models for this framework from data that was collected from a variety of places but one of the sources was the same survey data that University of Illinois Chicago collected. There is actually three waves of that survey that were done nationally to identify the buyer supplier pairing and the supply chains for long-haul movements. The other surveys were used were five Metropolitan area surveys collected in taxes for five urban areas and these were focused on the pickup and delivery system for trucks. In each of those five urban areas so we used those primarily for the regional models. These models were estimated separately for each model component for both food and manufactured goods and this allowed us to understand the differences between the two commodities and how that might impact coefficients in each of these models. Eventually of course we would want to estimate models for all of the other commodities as well. And in a place like Chicago as Monique mentioned it would be great to have local data to estimate those. The application was developed by starting with the work that Cambridge systematics did for the CMAP freight model and then we added the new components and program the whole thing in R because it's an open sources physical programming language that is freely available and served the purposes that federal highways had are making this readily available to other and POs across the country. CMAP is now of course using the software as are we and I see last slide is incorrect. The final report has been accepted and submitted to federal highways and is now available for use. A few pieces of information about the application. Kermit mentioned the total runtime at about 90 min. on a standard machine. This gives a breakdown of the runtime in terms of the various different model components. The regional model components at the tail end of the process really are quite fast because you're not dealing with the entire country so there is just a lot less data to process. The program of the national scale are where most of the time is spent because we're really synthesizing 8 million firms and trying to link them up and that takes a while. The mode and path selection is a good chunk of the time because it is a complex model with a lot of details in it. 
>> We also are implementing this framework now in the state of Florida that is in progress. Florida was interested in developing a statewide multimodal freight model and became interested in the framework and the -- how it addressed some of their objectives. Which include being able to represent the different characteristics of firms and shipments specifically at a zone level rather than at a larger level which is how a commodity flow databases are created. They also wanted to represent the complexities of the supply chains and the distribution channels and so this was an element of our framework that was useful to them. They wanted to understand how to connect the long haul and short haul demand and will be eventually working with the regions inside Florida to build out the regional scale portion of the freight forecasting framework for each of the regions in Florida. So their interest was really in how to better provide information for the regions inside their state. Of course representing all four modes. The other elements that was quite important to them was this concept of being able to capture trip chaining and representing that in terms of the freight forecasting. Both for the long haul supply chain movements and for the pickup and delivery of goods that occurs inside a region. The policy sensitivity for the Florida statewide model is really twofold. There is a regional set of policies and a statewide set of policies that are important here. In both cases the state was interested in promoting a framework that could address both a statewide interest and the regional interest. So at the statewide level they wanted to adequately represent the economic and land-use distribution of different industries and clearly there in the business of being able to estimate impacts from transportation supply as Kermit talked about a number of the different kinds of supply changes that would influence demand. They are very interested in being able to understand the distribution centers and their influence on commodity flow. And being able to test new facilities or even moving existing facilities and to see what impact that would have. And finally they it -- the influence and changes in commodity flow on things that are happening outside of Florida and what that would mean for them. On the regional policy side they want to have more of a direct relationship between the regional model input being an output from the statewide model which is why the integration was necessary. That is allowing both of the region and the state to have the same kind of sensitivity in terms of evaluating investments. There is clearly a more detailed distribution of employment and land-use that is affecting local truck travel patterns and that is important at the local level. As well as I would say smaller scale changes in transportation supply that nonetheless affect both long haul and short haul movements. And finally some of the local policies that they wanted to test such as identifying truck routes and specified truck provisions. Using delivery when does as a way to affect congestion in various places. And then wait and size limits on the network. 
>> So that is the end of my presentation and I am going to turn it back over to Bryan for the day. 
>> Thank you Mehran. I am going to bring up our evaluation pole for the remaining participants who -- to fill out. It will take less than a minute to even fill out. While folks are doing that maybe we can jump into the question-and-answer. It looks to be about five or six questions that were posed during the session and these in particular were posed during Monique secession but the first question Monique was it the pipeline mode was included in these networks. 
>> No, a pipeline mode is not. But that assume that -- that is assumed to be fixed. 
>> Moving down the line here. How do you choose between completing -- completing class I railroads. 
>> It's based on availability partly so all of the zones are connected to a nearby class I railroads or if there are a couple of class I railroads that hit a zone and then they can choose between the two. So it comes partly down to the travel time savings but there is also a random component in the path selection because we recognize that sometimes there is contracts or others with the partnerships that we have no idea about and no data on so the random component is intended to capture things like that that we can't see but we know are in place. 
>> Another question was for the participants was asking for clarification as to why fat-free was used in the study. 
>> The main reason for that is that factor III is more of the production to consumption and data set. So that is it has where the commodity is produced and then where it is consumed. And that is due to the sampling framework. There were wholesale firms included in the sampling Primark so there are going to be some firms that are in both producing and -- or look like they're producing and consuming without doing anything to the product. But basically in contrast with transfer data at which has the broken down trip legs transfers uses information from the commodity [ Indiscernible ] as well but than they supplemented with also with of information on individual track movements. And that is very useful for a lot of regions however it is preferable for forecasting for the model to be able to break down the productions consumption flow into these individual trip legs so using the faster data we have put the raw production to consumption flow and then we can break down the individual trip legs and predict exactly where it is and where the goods are going to be stopping in the region for some logistics handling. So that is in a nutshell why we chose that. 
>> The other thing I will add to that is that one of the benefits of trend search over at they asked has always been the disaggregation it provides to the user directly. These models both the work that Cambridge Systematics did and RC did are actually identifying that level of detail in the modeling system. So it's not necessary to start with -- it is actually not beneficial to start with a data set that has it broken down that way. So it is one of the other advantages.
>> Exactly. The power of the model is that it can break down the type into these different trip legs. And trend search Astarte dense you can't -- it is just not suitable for the same kind of forecasting process. 
>> One of the other questions is how truck load is determined so light heavy versus heavy heavy truck. 
>> I'm sorry percent heavy versus percent heavy heavy? 
>> The question was how does the model determine truck load. So percent light heavy versus percent heavy truck. 
>> The truck modes were less than truckload, full truckload and then truck with container. And the way we did it is and again we introduced a lot of flexibility so that the parameters could be more calibrated or Incorporated of more of a variety. But the way we did it in this version of the model was basically had for -- regrouped up commodities into four different types of ranging from bolt to finished goods or maybe it was just the prototypes actually and they all had a slight preference or different type of mode. And so that is captured in the model formulation. And also shipment sizes with a full truckload obviously it is only really advantageous or cost-effective to ship if you are shipping a lot at one time. And the formulation that we use as the shipment sizes as part of the cost. 
>> I will add that when we did the regional truck touring models we were able to get more specific and we identified the truck type in terms of the specific size of the truck. Based on the number of tires we had small medium -- light medium and heavy trucks based on a variety of factors. Those were multinomial models. But that is more detail than we had -- that we estimated at the national level. 
>> Another one of the questions was how you were able to develop it were model using commodity flow databases where records show dealing slows. 
>> I'm not sure I completely understand but the reason we used FAF so we had links flows to begin with. When we get the truck touring models in the modeling system we know where everything came from. Unfortunately all of the commercial vehicle surveys that we had access to were not linked surveys. We don't necessarily know all the details of the long haul supply chain for the trucks that we were estimating from the surveys. And we did identify that as something that would be useful to change in the future to have surveys that could identify shipments all the way from production to consumption including all of the pickup and delivery system at the end of the process. And since we didn't have that in terms of being able to estimate it that way there is potential disconnect in terms of what we estimated but the models were intended to be truly integrated and in the future as data becomes available we will be able to reestimate those models with those true linkage. Hopefully I answer the question. 
>> Please do enter -- follow-up question into the QA pod if your question hasn't been answered or wasn't interpreted. 
>> Another question is where the information on estimated travel times and cost by mode throughout the country was obtained. 
>> I can answer that one. Basically we developed the so-called stick network which was based on information from Oak Ridge national labs and Army Corps of Engineers. And basically for this round we assumed a speed for each of the modes. And used that to come up with an estimated travel time. 
>> And then for the different types of logistics we assumed different parameters that are similar in nature that are prime to handle things like that. 
>> Another one of the questions is how this model could be integrated into typical regional models and the questioner particularly says a commercial package such as trans-cab. 
>> I can take a shot at that one. I made an initial point that it is a completely different animal than a traditional model but a number of the data outputs can be aggregated into a traditional trip table format particularly if you want to assign it to -- you want to have this contribute to your regional travel demand assignment. You can produce a trip table that would be associated with a particular class of trucks. We're in the process now of working through which -- what kind of adjustments we have to make to our standard QR FM approach in order to accommodate the better information that comes from this but this doesn't completely replace and doesn't intend to completely represent all commercial vehicle movement that would appear in a traditional model but it is possible to swap and parts of it. 
>> So some examples of what it doesn't necessarily represent would be trucks that are not carrying goods like service trucks. These can be estimated using the same regional truck touring models but you need to identify the demand which would not come from a commodity flow data set. It would have to have a different source of demand and then you could use the same truck models. So there are a few missing pieces. 
>> This is a question for you Kermit Wies or whether the software coded in R described in this webinar is available. I presume they mean if it's available to be shared. 
>> Yes. We are willing to share what we have. As I mentioned at the beginning we are trying to develop a community of users to help maintain and improve this project so what I would ask that you contact me directly so we can talk about what you have in mind and hopefully bring you into the full -- bowl gently. It is not commercial software. It's not something that will work so well that you can just swap in your own data set than expected to work. You are going to have to weigh it in with the rest of it but yes it is available. 
>> The user can -- community now includes people in Florida were also very interested sharing and using and growing the community so the software is maintained in a way that everybody can gain other people using it.
>> How does CMAP plan to use this model day today after you comment on the relevancy with your planning process. 
>> The genesis for this was either the development of our long-range plan a lot of questions being asked about the relationship between our freight industry cluster in Chicago and economic development potential that could be built into our plan. Right now that is day to day use. We have freight committees and policies and programming decisions we make. It is not used in any way like a traditional network model to decide how big and intermodal facility should be but it is used to provide more general cost benefit information when we talk about possible public investment or possible regulation of freight related activities in the Chicago region. It's more to more uses a planning and educational tool now which is why I don't lose too much sleep over the fact it's not completely validated and properly estimated model. Right now we all industrywide have a lot to learn about what these models can tell us even in their current form. 
>> There is one final one here I think is a good one. What is the budget for these different projects. 
>> I guess I don't really feel comfortable answering that on a recorded webinar. Is that a common question for me to blurt out? I willing to share that information with an individual who is trying to manage one of these types of exercises so feel free to contact me and I will talk to you about how much we budgeted and I will even share war stories with you about how it was to secure the money. 
>> I think that is a completely acceptable response Kermit. 
>> I don't think there any more questions coming in so I would like to thank Kermit and Monique and Merrin for taking the time to prepare and deliver this webinar for sharing the experience. If you have any questions or comments about the material presented today please feel free to e-mail us at feedback@[ Indiscernible ].org. TMIP.org. I want to remind folks we have another webinar scheduled for Thursday February another webinar scheduled for Thursday, February 21 in the same timeslot where the Houston Galveston area Council will present their experience developing their new activity-based model. And that will conclude today's webinar so thank you everyone. 
>> [ Event Concluded ]