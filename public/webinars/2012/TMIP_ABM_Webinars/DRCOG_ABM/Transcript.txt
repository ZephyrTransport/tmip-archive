>> Good  afternoon everyone. Welcome  to today's TMIP webinar on  aged me -- agency experience with  using  activity-based model. My name is  Brian Grady and I'm a contractor  with the team at the outreach and  I will be moderating. This session  is  the third in a new series which  is an addendum to the  recently completed 12 session instructional  series on  activity-based modeling. In the  series we will showcase agency experience  using a be models by demonstrating  how  the research theory and concepts  presented in the preceding series  are being put into practices by  agencies across  the country. Our third session will showcase  the Denver regional Council of governments.  Presented by  Erik Sabina and Tom Rossi. I have  a few administrative items to mentioned  before  we begin. In order to provide the  best possible  audio experience we have automatically  and remotely muted all participant  telephone lines. At the end of this  webinar we will be posting a brief  questionnaire. If gas the team is  interested in  your feedback. So please take a  92nd break at the end of  the webinar before the final Q&A  section to allow time to complete  this  evaluation pole. This webinar will  last approximately 1.5 hours. We  will  have roughly one presentation followed  by 30 min. period for question  and answers. You can submit questions  via the webinar meeting room software  at Q&A  pod window is displayed on your  screen and you can enter questions  there. They will  be presented during the Q&A session  and we will have  one session between the model development  and model application components  of  the webinar. We're recording this  webinar and will make it available  on the online community of practice. This  webinar is also being live closed-captioned  for the hearing impaired. If you  do not wish to view the closed  captions click the full screen button  in the top right-hand corner to  maximize the slide  you only. -- Slide view only. The information  in this webinar is made available  for knowledge and experience sharing  purposes only and does not represent  an endorsement  by FHWA.  

I'm pleased to announce today  speakers, Erik Sabina is the regional  modeling manager at the Denver regional  Council of governments where he  has led several leading-edge modeling  projects including the development  of an activity based travel model  for the [ Indiscernible ] region  and the first regional travel survey  to cover the entire Colorado front  range area, and the ongoing effort  to develop an implementation  of urban as I am for the Denver  region. Tom Rossi is a principle  of  Cambridge Systematics with 30 years  of experience in transportation  planning  and travel demand forecasting. He has developed  and applied trip based in activity-based  models throughout the United States.  But the past 20 years Tom has been  a consultant to U.S. DOT for model  improvement research and development  teaching and  training courses. He is also the  chairman of the TRV committee on  transportation demand forecasting.  And so Eric please take  it away.  

Thanks Brian and my apologies  to the listeners who got to hear  that amazing feedback due to my  having failed to mute my computer  speakers. It sounded from my  and like a soundtrack to a  science-fiction movie. I hope it  wasn't as loud for you as it was  for me.  

We have spent time trying to  figure out an outline as we were  contemplating how we might go through  this presentation and basically  we  wanted to start from the beginning  and go to the and and to make a  long story short talking about where  we were at the  beginning and sort of finishing  up with some other work that we  are doing associated with the development  of the focus  activity-based model. So we will  -- and attaching steps along  the way of how it was we decided to do  the project and in what way and  how we built the model and how we  are maintaining it and some of  the applications that are being  used for now. With that I will  blast through. The first slide  here basically its purpose is to  let people know that we were not  in  any special advanced state when  we began  this project. It was mid-2001 we  were running a based trip  based model and what I described  as a place-based a  household survey that is been taken  in 1996 and 1997. There's been a  lot of model or turnover at the  time I came on staff so I was the  only model are here at  that point. We took several luminaries  steps before embarking  in earnest on the activity based  model phase of this whole thing.  At first we completed transferred  to trend Which I had been in process  when I came  on board. Proceeded to a visioning exercise  for  the ABM side. And finally began  the actual work in earnest in December  2005.  Division says -- we've had discussions  with some of the federal highway  folks and they suggested that people  might be particularly interested  in this. It is sort  of interesting. We  were very new at the time in  working on  next-generation models which is  a gross understatement. We knew  very little  about them. And there were very  few in existence at  that time. The only ones I know  of that were operational when we  began where  the SFCTA model and a couple  in process -- and why MTC was operational  at that  time and MOR  -- MO -- MORPC in Ohio was part  way through development. We didn't  have a lot to go one. From in terms  of on the  ground experience. The vision phase  had a couple of purposes. One of  the key ones was to  build support that we wanted to  make sure that  those people out there in the community  who could have influence on the  success of our project as we went  along would be supporters.  And then we wanted given we knew  very little about next-generation  models to use a  visioning exercise as a educational  tool for ourselves as well as for  other people and it proved to be  that way. We went through a lot  of steps in the vision exercise  and we weren't sure how far  to go. How much we needed to do  in the vision exercise and I would  say we're probably aired on the  big side that is to say a lot of  meetings  and participants etc. We even held  a public open house at Denver  Public Library that is across the  street from  our offices and I still remember our executive  director was amazed we got 120 people  show up.  I believe his phrase with the nerds  were taking over the world and he  was  just floored.  >> We established three panels. We  had an expert panel with these names  that you see here. All of whom are  rather well-known in the business.  Some of them extremely well known.  They of course were very helpful  to us in thinking through what we  had to do an  understanding better some of the  projects that had gone before us  and were already in process. We  had several meetings with them,  presentations made by our  lead consultant,  PB consultant. We hired them to  help us think of  this through and they did a lot  of  excellent work in reviewing existing  models in helping us think through  the theoretical structures that  were being developed at that time.  And making interacting with the  expert panel and so on. The  technical panel was composed of  what you would expect. Basically  technical level people  from all organizations that would  have interest in this sort  of thing. Including both governmental  groups and nonprofits and private  sector etc.  

We had a policy panel and this  was one of the  key aspects of building support  that we really wanted  to have as high level people involved  in this as we could get with the  hope that we could win them over  to  the idea. We had people as far up  as the deputy director of the Colorado  Department  of Transportation. We had a representative  from a homebuilders Association  and a home-building firm  that builds -- done a lot of work  here in town. We were trying to  cover  the spectrum. It was very interesting  to do and it did  have the effect of building support  and it also -- we got a lot of out  of them in  terms of what it was they really  wanted to see from  such tools and what it was they  were not getting from the previous  generation of tools and what they  hoped we  could do with this project and building  a  new one. 

As I said earlier we weren't  sure how far to take this dialogue  so those of you who are considering  doing this -- I would say give me  a call when  in doubt. We probably went a little  more -- further than we  needed to. But it certainly didn't  do any harm to spend more time at  it then we probably  had to. We spent a lot of time talking  to the customers  policy needs. Especially trying  to figure out how to explain the  models in English  that people are -- laypeople in  our business  understand better and I really can't  stress enough how important that  is and I'm sure many of you perhaps  all  of you have come to that same conclusion  when you're trying  to explain what we do with other  people. I will  say that telegraphing my punches  here that after having worked with  our activity based model and application  world for the last couple of  years now I am continuing to be  a big fan of the things that feel  that they produce  wonderful results that are really  opening up areas of analysis.  I did -- one of the things that  usually say is it was our old model  -- it was ever present and frustrating  experience for me when people would  ask can you  do X and I had to say no a lot.  And now I have to say no an awful  lot less frequently and that  is gratifying to both the customers  and  to me.  

After having gone to the vision  phase we then how old up with  technical  team at Dr. Krog and proceeded to  draw a lot of box I grants. You  are not intended to  read this. This astutely here for  [ Indiscernible ] value. Hope everyone  is laughing at  this point. A lot of box diagrams and more  box diagrams and  even more. Even more box diagrams  and then someone got creative and  decided that they were going to  use this as an analogy for the model.  I had to go -- do with power  and  speed. Eventually we developed a  model and towards the end of the  project decided to call it focus  early on. And I believe this is  where  I let Tom take over and describe  some of the details of the design  of the  model.  

Thanks Eric and  hello everyone. We were hired  by DRCOG to develop the model and  the first thing that we did was  to do a model  design plan that was intended to  make sure that the model met the  needs as well as possible  for DRCOG.  It included going through a process  with them and  with the -- there other stakeholders  to determine what the model needed  to do and what kind of analyses  would be used  for and considering things like  constraints of the agency and what  they had in terms of resources for  developing  the model and running the model  and validating the model. So constraints  on data and  on staff and on time and  money. Hardware. All of those things  needed to be considered. And as  Eric mentioned there really weren't  very many models in place at  that time. So one of the -- what we ended  up doing and we had on our team  the developers of the Sacramento  model -- Mark Bradley and John Bowman  and so we developed a design that  was similar to  the design for the Sacramento model.  The model they had  called DaySim. That was still in  the process of being incremented  in Sacramento. It isn't exactly  the same as the  Sacramento model but there are a  lot of similarities in terms of  the  model structure. There -- the  dates and approach was based on  work that John Bowman had done at  MIT  with Bowman/Ben-Akiva on a pattern  approach  and mostly activity-based models  in the U.S. and have a similar approach  to this. They have branched off  and on in a lot of different directions.  I'm not sure that there  are any to activity-based models  in the U.S. that are exactly the  same. There are  a few -- there has been a lot of  work recently in transferring models  but  this was not a transferred model  and it was all estimated using DRCOG  recently collected -- not  recently  -- collected in the  late 90s. The data they have collected  in household survey dated that was  used to estimate  the model.  

So this is a diagram showing  the structure  of the model. The model as you might  imagine is  more complex than this. We aren't  going to go into great detail on  this  because Eric and I and others have  presented this in  other venues. There are a  lot of documents out there that  can be used and certainly if people  are interested in the structure  of focus  please contact area Courtney and  we would be happy to give you what  information we have to talk with  you about  it. Basically the blue  boxes are data and of course the  main to types of data that are used  in any model  are inputs and social  economic data, and network data.  And social economic data in  this case -- a lot of it was at  the zone level. The model has about  2800 zones. There is also a lot of parcel level  data and we will talk about that  more in a few moments. On the  network side the networks were maintained  in trans CAD and so they  are pretty  conventional networks. Those are  the two main input  data sources. For the model and this is to apply  the model. Obviously there other  data sources like the survey did  I mention that are used in estimating  the model. The green boxes  represent groups of model components  and we don't show every model component  here  mainly because there are so many  of them you couldn't fit them on  a  screen. But you can think of it  in  terms of the long-term choice models  which are choices that are not made  every day and these have got to  be a standard set  of models for activity-based models  that have been developed subsequent  to focus. They include vehicle availability  or  auto ownership. Regular workplace  location for every worker that is  in the  synthetic population. The regular  workplace which was asked in the  survey  is model. The school location for  any students in the  synthetic population. I will talk  about the population synthetic generator  in a moment. Those of the long-term  choice  models and in Focus. Once those  around we have a list of persons  and whole sold  with characteristics and we go into  what we  call it person day model where we  estimate the daily activity patterns  of every person in the  synthetic population. This is a  list of activities that they're  going to perform and they are formed  into two orders  and activity-based models as I'm  sure you know by now all too  were based and the purposes of the  two words and the activities and  some of  the tours have multiple stops  on them. We have a list a person  two words and the purposes for the  stops and we go into  the tour models which look at the  locations of the primary activities  for  the tours and the primary mode of  the tours and the start  of time and end time of  the activity. And the  specific stops that are generated  as part of the models. So after  this we have the list of  the tours with all the purposes  and the destinations and the modes  of time of day for  the tour are attached. And then  we go into the trip models with  the stop models and that is were  remodeled for each of  the stops and their locations based  on the home location of  the traveler and the are ready model  location of the primary activity.  The modes for individual trips --  there are mixed  mode tours and sometimes people  may get a ride for part of the to  work and it may take transit for  part of it so for each individual  leg of  the trip the modes are modeled at  the time of day for all the stops  is model. So what we have at the  end of this is a list of all  of  the tours an individual trips made  by each person and this  is where -- we aggregate them to  trip tables for use on Highway transit  assignment. There are some integration  of the model components as shown  on the  left side. Lock some variables are  used  to feed information back to previous  models. There is a feedback loop  on the right  side that --  the model isn't run once all the way through.  Like models and a lot of congested  areas there is a feedback loop that  beats back the travel times and  the models  run iteratively.  

This is a summary of  the components. The  population synthesizer which again  I will talk about in a moment and  those are some of  the main components that are used  to generate the data.  The three -- numbers two and three  and four the long-term  choice models -- the daily activity pattern models  and some of the components there,  six and seven and eight are the  two or level models I mentioned  and nine 1011 are the -- nine and  10 and 11 and 12  are the triple level models and  the  assignment  process.  

So just to give a quick background  on the daily activity  pattern models. And unfortunately  I  have lost my network connection  and this is  happened before. So hopefully I  will get it back in  a moment. I am  pulling up the slide on  my computer so I'm assuming everyone  can still see it on there. Brian  and Derek?  

We are good.  

I'm going to put them up on my  computer so I can still speak to  them so I know what the  slides  a. Hopefully it looks like I getting  reconnected  here. Sorry. As I mentioned it is  a variation  on the Bowman and then I  keep that approach. It is a series  of multinomial  logit models. We model it by different  types of people because they have  different types of activity characteristics  for example a person  type includes full-time workers  and part-time workers,  nonworking adults, children who  are generally students of various  age groups, older  people, seniors, and university  students  as well.  

And what these models do is predict  the number of tours for  each purpose that each person  will make. And then whether there  are additional stops of various  purposes. If they are decide --@get  to that two words but for example  there may be a person may have  a  work tour and a shopping tour and  they may have a  meal stop. 

You can see the list of activity  purposes that  are used in focus, work, school,  escorting which is a tour or a stop  that is just for  the purpose of dropping off or picking  up a  person. Neil shopping and social recreational  and personal and business. Is the  same set of activity purposes where  they were talking about the main  to her purpose or the  stop purpose.  

Once we have that we look at  the intermediate stop frequency  which then predicts for  each tour the number of stops by  purpose. Since we are do you know  what stop purposes  there are we are able to then generate  the number of stops on  each tour. That is a multinomial logit model.  There is a frequency model. Some  tours for this model  are generated at the workplace by  workers so they come back to the  workplace so  they start and end at work. It could  be something as simple as a worker  going out to lunch and  coming back and it could be someone going to  a meeting or someone going out for  personal business but as long as  they come back to the workplace  that is a work  based tour. If they leave the workplace  and go somewhere else don't come  back to work we presume they go  home  and so that is actually part of  the work to work because they left  him to go to work at  some point.  

I wanted to talk a little bit  about the time of day models.  It was decided early on in this  process to do the time of day at  the our level of resolution which  means that there are  24 alternatives. When remodeled  the activity pattern at  the tour  level we started with join in and  time jointly  and so there were more than 24 alternatives  because we have to model the start  and  end time. The only real constraint  is that the end time has to be after  the  start time. And because people can  have multiple tours we have  to model -- we have to model the  two were so they don't overlap and  they don't take place at the same  time  so we model higher priority tours  first which are things like work  tours and block out this times so  they are not available for  other tours of the  lower priority. At the trip level  that is the individual stops and  we basically model backwards  from the primary activity time back  to the homeland for  the trip made between  the home going to the primary activity  location. And then  we model forward order when coming  on their way back home from the  primary activity.  We know the in transit time because  we have the network scans so we  are able to basically  model the -- departure time  and duration of  individual stops. I will note that  most activity based models that  have been done  since focus -- Focus uses a finer  time resolution a half  hours. Now --  typical now. The new model that  is when to be developed in  the process in Los Angeles I understand  uses a  continuous time type of model so it is not a discrete  choice model because it is not using  discrete time periods so that is  a  different time of day model.  

Part of the reason it was chosen  as ours for this model  was because it was many years ago  that the computers weren't  as fast and certainly time of day  modeling is one of the  things that can take a long time  in an  activity-based model.  

I'm going to summarize some of  the key functional aspects of Focus.  As was mentioned it is estimated  using the 1996, 1997  survey data. The calibration  here  was 2005. And so there was a set  of validation data that  is similar to what is used in all travel models  both activity-based and tier-based.  Traffic counts, traffic ridership  and  so on. There was a back cast done  against 1997. And actually most  of the ballot -- almost all of the  validation work  was done by DRCOG staff once we  have the model up and running and  we developed a validation plan  early on and then it was basically  something that  was followed by DRCOG with a little  help  from us.  

All of the trips -- this is a  tour based model. We have it  work based  sub tours and all other tours in  the model our home base  to tours.  

I want to talk briefly about  location  choices in Focus. We  have both zone-based and parcel-based  or point-based  location choices. The main Destination  Choice models like workplace and  location we do want to get the zone  level first and a lot of  things depend on network  scans as I'm sure you know in  Destination Choice trip  distribution models. The zone to  zone skins are a important and we  can  have any -- we can't have matrices  of parcel to parcel level scams  and they which is the enormous or  John  or Ms. as I guess the next generation  calls them. So we have the parcel  locations and these maps show examples  of the parcel locations that are  in DRCOG database. We do the location  choices at the zone level but then  we choose a  point level within the zone using  Monte Carlo simulation to get to  the  point level -- the actual point  or parcels of the locations of the  activity locations. In  this case we do use information particularly  for short trips which are almost  the  nonmotorized trips. But there are  also used for  transit access so we get more accurate representations  of those things that you can get  from just  using zones and their  associated centroids. Because activity-based  models are aggregate models when  we are modeling  every person we can actually choose  different locations within the zone  for  each person based on the Monte Carlo  simulation and that provides with  the way the getting at these more  accurate estimates and short distance  and transit access  and egress times  and distances.  

The Mode Choice model has  eight votes. School bus as you might  imagine is available for school  tours and some of the other modes  are not available for every two  were for example the transit modes  are not available for as  court tours. And  escort trips. The other modes  are generally available for  most things. As I mentioned  for modeling, bike and  walk trips land-based use was crucial  to preventing  this capability to use accurate  input information. DRCOG  doesn't have bike and pedestrian  networks yet but they are exploring  the feasibility of using those networks  and  since Erik is a bicyclist I'm sure  that  will happen.  >> 
     I mentioned on the flowchart screen  that there is integration  of model components and Logsum said  different types  are used in accessibility variables  and models that are  run before Mode Choice.  In general you may be  familiar with even enforced step  models and more  choice Logsum being used as impedance  variables and trip distribution  or destination choice models of  that same kind of concept but as  you can imagine the flowchart didn't  show  every model but several models in  each of  those boxes just doing  complete logsums is everything below  some of the upper-level models it  would have been feasible so we  did use what we are calling  aggregate logsums  for various components of that is  something that is probably two,  located to go into in detail now  by DRCOG does have documentation  of that that  I'm sure they would be able to share  people wanted to hear about that.  Generally for  models downstream or model supplied  after two or  more would -- to her Mode Choice would use  generalized cost variables. So the  trip choice model which happens  -- happens  after the two or model uses a generalized  cross variable that is basically  a part of the utility function in  the two or more it model that has  to  do with the level of service of  the time it  costs variables.  

The structure of that is consistent  because now we  are using the same weights on in  and out of vehicle time or the same  weights between time and cost as  in the two were  mode -- tour mode  choice model.  

I mentioned the population synthesizer.  It is called PopSyn and  developed by John  Bowman and for the Atlantic regional  committee and Atlanta generously  gave their  model -- to DRCOG for use in  this project and John Bowman was  on the consultant team and was able  to help them implement it and that  generates this pathetic population  for the model. -- Of  the synthetic population. It will  generate a list of people and their  locations and things like that.  That they and all their  characteristics  and households. I did want to mention  -- we often  called the exogenous trips and activity-based  models because they can't be estimated  based on the surveys. The household  surveys. So things like the external  trips and internal and external  and external external. Commercial  vehicle  trips and the  airport trips -- there are airport trips in the  household survey but only for the  people who live in the region and  Opera people who are flying in and  out of the region. To  the  airport. DRCOG already had these  components in place from the trip  based model and continues to  use those  in their activity-based model.  

The way they do this is they  basically run the whole trip  based model inside the tour based model and  these components are used to create  the trip tables for assignment  for external and airport and commercial  vehicle  trips.  

Finally I wanted to point out  what we call tricky variables to  create for  future years. One of the variables that is used  in several of the model components  is intersection density with a number  of intersections within a unit area  for  a zone. I think that is something  that can be easily  measured using GIS for the base  year but in forecast year in an  area that is growing as fast as  Denver there are new roads  being built and new intersections  being created so we don't know exactly  what the intersection  density is. That has to be estimated.  That is a  tricky variable. Another thing that  we do is we have a school location  model as we talked  about and we know we're -- where  all the existing schools are and  DRCOG had the database put together  that had very comprehensive in that  regard. But again since the region  is growing that means there is going to need  to be more schools opened so they  need to be located in the future  as well. That was something else  that had to be done without  any real data to  do it.  

So that is an overview of the  model itself. I will turn it back  to Erik who will talk about how  it got built.  

 Thanks, Tom. This slide  you see that this 50% DRCOG and  50% consultant is a bit on the approximate side.  It was my best shot at it thinking  to the amount of internal personhours  that were spent and knowing  much more accurately however  how much money was spent on the  consultant team. But  we had very different sets of responsibilities  on the project during  development and so this 50-50 thing  is the cumulative total of what  the  consultant team versus did what  we did. DRCOG as I suspect is  fairly common in projects of  this sort. It was sort of at the  beginning of the  critical path to develop all of  the basic input data  for the estimation data sets. So  we produced  many skim tables and we provided  our survey data and many other sorts  of data that were  needed eventually to assemble the  estimation  data set. That was a great deal  of work. We spent basically about a year  doing it and we had a whole lot  of variables. It was a project  that required in our case we probably  had around a dozen people working  on it at various levels of the time  and not full-time each one of  course but I have a spreadsheet  that I created and this is a subset  to track the progress of all the  various people who were assigned  to the various  data items. That worked out quite  well helping us keep track of what  the status was a beach thing and  where everybody put their data after  they had developed  it. So that  was helpful approach. We did side  trips. I to know if other people  can read this but at one point in  the middle of it we  realized that we wanted to spend  some  time verifying the accuracy of the  transit paths that the  path builder as it was constituted  was creating and I'm sure that many  of you have gone through this exercise  even when you are not building new  models but trying to update the  ones you have. We  ended up spending lots of time  on this and I'm sure Tom would echo  that  any topic that either of us are  discussing today we can spend at  least  an hour on that individual thing so given  that we don't have that much time  we will have to forbear doing that.  We did spend a lot  of time and probably delay the project  a little bit in the process but  it was interesting. It ended up  with [ Indiscernible ]  talking about the approach that  we took to verifying the accuracy  of the path builder and  making adjustments.  

After we had gone through all  of that process  and provided the data to the consultant  team in a very  poly lock file with some things  being databases and some things  being spreadsheet and some things  being transcribed and TX file and  on  and on they went through quite a  sophisticated arduous  process of reconciling all of that  data and putting it into a form  that would be  needed for the estimation process  which was conducted in  a  legit. --  Of logic.. They did the last  of information and DRCOG did some  as well. Which I will talk about  in a second.  So ultimately a  number of estimation data sets were  delivered and visit not -- this  is not expected to be able to reader  that but it is an excerpt of a dictionary  that accompanied the  estimation data that was used by  the consultant team and by DRCOG  and some of the estimating  we did.  

The consultants  basically developed work on the  model design. We did  a little [ Indiscernible ] along  with it and came up with a decent  idea. But occasionally  the Cambridge -- Cambridge  Systematics team and Bowman team  are leading the charge to  doing almost petty brain lifting in deciding  what the model structures would  look like. This is  a version of the flowchart that  Tom went over earlier so I want  bothered to talk about it except  to  say that this flowchart which conceptualize  our design was created early on  in the project and we  remained remarkably faithful to  it throughout the rest of the project  so I guess that  speaks well to the thinking that  went into it by the consultant  team upfront.  

As I mentioned a little bit earlier  we -- it's worth backing up and  saying that our philosophy that  was we wanted to do as much work  in house as we could  consistent with the knowledge that  we had the knowledge we had and keeping  schedule. The basic goal was one  I suppose is well-known to nearly  everybody on the call now that we  figured that if we did a lot of  work on the  project then during the development  phase was would understand  the product better when we got it  and be better able to maintain it  and use it. And that has proved  to be the case in spades.  Hardly recommend -- I will throw  in recommendations here  and there. As Bryan said, this is not considered  to be the federal Highway administration's  endorsed view of things and it's  just  me talking. I hardly recommend you  do as much work in house as  you can. You will get the benefits  that I just mentioned of understanding  the product better when you get  it and if you are the nerdy sort  of people out there in the world  and I suspect you are as I am otherwise  would need to be listening  to me you will probably have some  fun doing this and we certainly  did  as well. We estimated basically  the long-term models and I will  go into too much detail why accept  to  say that working together with the consultant  team is we estimated models that  were not on the  critical path of the estimation  of the other components so that  if we were slower having problems  that we would not get in  the way while the consultant team was estimating  the bulk of the other models. You  can see we did workplace and school  location and Ottawa availability  and altogether  those constitute about six or  seven models in the consultant team  estimated all the rest of them which  probably is closer to  40 models I would say. I haven't counted  them but I think it is in that ballpark  when you consider several models  for separate trips and to  her purposes.  

So that worked out  very well. I think other people  of heard me say before we  actually engaged one of the consultant  team members as basically  a coach to help us when they got  stack and  get us -- drag is that of the ditching  get us back on the road again when  we were doing the estimation and  that was a small contract and it  worked extremely well and was helpful  and really fun learning experience  that was a vision in terms of getting  the project done in a timely manner  so I recommend that way of doing  things if you  do decide you want to do estimation  on  your own.  

DRCOG did most of the  estimation work. Tom was  shocked because his can hit -- and  his IT group had spent something  close to  a year of time on  their own. And I thought about it  for a while and said well I had  one of my staff spent pretty  close to  three person-years just on her own  and was  pretty much all she did was she  was working at DRCOG for three years.  It was a big effort to  do software and I do want to digress  slightly and emphasize that you  all should not be turning pale of  losing your lunch if you are thinking  about doing  this. Because we did take a  very individual and from  scratch approach to software development  which was a large load on the project.  And I don't see any reason why other  people do -- other people need  to do much of that giving other  models  out there and to pirate for your own projects.  I don't think there's any need to  go to the amount of interesting  pain but they nevertheless that  we went through.  

I  would say that 80% number is accurate.  DRCOG in-house and IT leads did  a lot of the  conceptual design and I referred  to it as the plumbing of  the model. That is essentially all  of the controls aspects of the  object-oriented  design. Ultimately it pass  data around and built a model components and  to get out and put it where it is  supposed to  be stored and passed new data to  the next component and so forth.  And interact with the graphic  user interface that is the face  of the model for the users and  the world.  

That was as I said very arduous  but interesting. The Cambridge  systematics team of IT folks was  really crucial at the end in taking  this thing from  a reasonably well sanded thing to  a nice shiny product that look  nice there in your  living room. It worked out very  well. I do want to mention briefly  that  speaking of the way the contract  was written. We essentially  assigned Cambridge systematics a  number of the components and I'm  speaking of  that box and arrow diagram that  Tom went  over earlier. And said these ones  are yours that you guys will estimate  them and you will also build the  software for them. And  then DRCOG Several of them on our plate of  the ones that of our dimension.  The work and school location and  Ottawa availability. We would develop  the software for those. Basically  how that worked was  that DRCOG build this plumbing I  mentioned in that we also built  a  decent prototype of the legit --  Alogit  solving component as mid-May of  these models that is what they do.  It's one form or another of a discrete  choice model. We built  that prototype and then we assembled  a team of several folks from Cambridge  Systematics IT group and several  people from our modeling  group here and they eventually rounded  themselves into a distributed model  development software software  development team. And that is  where Cambridge IT specialists came  to  the floor and provided very specialty  knowledge that was crucial to getting  these things -- performing well  and running fast. And  then they concentrated on the components  that were on their part of the contract  to do and we concentrated  on hours and where we all came together  was that we're all working off  the same software object-oriented  framework that ultimately  solved logit problems. That was  something that worked well and  the teamwork -- team worked  great together. This is a very simple  conceptual design of the  overall plumbing where there is  a little user on the left and the  graphic user  interface and several components  of that essentially they control  how  things happen is what we call a  request to dispatcher that takes  requests from  the GUI. The skinny line you see  between  the July -- GUI and the dispatcher is an  elegant approach. All that is is  you create an XML file and  the GUI writes an XML but has a  list of the components that are  to  be run and locations of input data  and locations of output data and  that  thing. That reconstitutes it requested  it is  past -- controls other aspects.  The model engine that you see there  is basically in  charge of taking the items from the request  dispatcher which of the list of  components that are to  be run and controlling their sequence  of runs and that there is  a database manager which is in charge  of interacting with where the data  is stored. The two basic places  where data is stored. In this model  our SQL Server relational  database and MTX files and as of  the Sandra  -- standard matrix files. Used in  trans CAD.  >> I'm getting to the part where I'm  just about to ask is got to say  one or two words. Scott is one of  the senior developers at Cambridge  Systematics but I want  to note those of you who are interested  in knowing what sorts of languages  we build this thing in. The core  of all the plumbing I mentioned  and all the logit solver was  written in  C sharp which is the best way I've  heard it described is Microsoft's  answer  to Java. That language worked out  well and it is a classic  object-oriented languages. A good  chunk of the model of course is  still built-in GISDK and all of  the  network processing and assigning  and so forth of the back and still  lives there. Microsoft SQL  Server database is the other portion  of data engine as I mentioned that  there is quite a lot of programming  done  in SQL in their in stored procedures  and views. And finally we don't  use job of her very much but Bowman  and Sun built the  pops them  population sympathizer in Java so  that is also in the overall  model Sean. This is just one view  of the user interface and their  a number of different slides and  screens in the interest of time  I watch of the mall. The main message  I want to get  across is that in a manner similar  to what I suspected most  of the existing vendor produced  travel modeling software packages  can do we  can also list the model components  that exist in focus and then we  can select subsets of those components  that we do not have to run the entire  thing every time I want to run the  model. So that  has been -- that was a deliberate  design choice that affected software  and other things in  great detail. And has been a very  helpful for us in a number of ways.  And has been very practical as we  started to apply  the model and running it for a regional  transportation plan.  

I will let Scott makes chip in  a little but here and  talk about the class and method  structure particularly of  the logit solver and this is a list  of some of the basic  standardized methods and I will  let Scott make a  few remarks.  

Hi this is got from Cambridge  Systematics. One of  the goals that we had upfront with  the software is to try to design  a very clean and elegant object-oriented  design so  we could reuse the most amount of  code because when you get down  to it at its core the structure  of most  of the logit models is similar and  so there was a lot of opportunity  for code reuse. In fact  our basic component Pro is what  is shown on this slide so essentially  you start  by preferring -- preparing the  model. You figure out the information on the  alternatives what the model you  were doing and the coefficients  are for the utility equations and  how those equations are together  with the alternatives for  the model. That you read in whatever  data you need whether this is household  data or  person data or level of service  data. You read all of that in or  in some cases you can also differ  that depending on whether this is  one of the big models where we have  to  work at [ Indiscernible ] things  that we didn't overtax our memory  and processes all at once. You have  a process where you  can read in a parcel the data you  will be working on. You run the  model which is essentially just  crunching your billions of numbers  and your tabulations for doing all  of the utility functions. I  think the because when you get to  doing the location choice models  where you are essentially having  to compare every zone with every  other some did you wind  up doing a lot  of calculations. But the core method  is to run the model and once you've  run the model you get down to a  point where you have chosen one  of  the alternatives. Then you write  out the information of that alternative  for  whatever the controlling factor is. For example,  if you are doing a vehicle  ability model then the run method  would be choosing the number of  vehicles for each household and  then you write the data  out and write the number of vehicles  for each household so it is available  for the next model down  the chain.  

This  is just  getting into the class a hierarchy  that we set up. This first slide  shows how the simple concept of  a variable has many different aspects  to it so a variable essentially  is something -- a piece of data  you are reading and that will fit  into some utility equations somewhere  and be multiplied by some coefficient.  But how that was done  depended on how the particular variables  fit into the  overall models . So there were some variables that  were specific to the alternatives  so for example if you are doing  a location choice model then there  are variables that are data about  the zones that you are looking  at choosing. Another type of variable  was what we called the decision  makers specific variable. The decision-maker  is this concept is we had what was  driving a particular  model so we were doing for example  auto availability and the decision-maker  is  the household. If you are doing  daily activity patterns in the decision-maker  is the person. If you are trying  to do two or time of day that the  decision-maker is the two were.  It  is what the driving data object  is. There are variables that are  specific  to those. To dealing  with large sums  -- logsums. We had special clays  classes to do  with those. The other major piece of the model  are the alternatives because ultimately  what this is  all about is setting up the utility  value so you have probabilities  that you could do a [  Indiscernible ] simulation on for  an alternative. There are two main  types of logit models we had to  deal with. Multi-no real  which is -- it is a  flat low-budget and every alternative  is to do the same but you are computing  a utility and ultimately a probability  for  each alternative. And there  is nested logit and in that case  you have to build up a hierarchical  structure of the  alternatives combining the information  about a particular nasty along  with whatever the alternatives were  in that master. So we  had to come up with a way to model  this in our [ Indiscernible  ] hierarchy.  

This  gets into the core of it and I will  go very briefly over this because  there is a lot of complexity. You  start with  this abstract concept of a component  which is essentially one piece of  the model whether it is vehicle  availability or daily activity pattern  or two or time of day. There are  all types  of components. The model component  was a way of making bad specific  to model and we had some additional  components that aren't shown here  they did things like move data around  and prepare things for  running models. And then under the  model component that there could  be multinomial logit or  nested logit and once we started  working with multiprocessor machines  we could start doing breaking out  the data and  running different chunks of data  on different threads on  different CPUs. There are more detail under their  but I will leave it at that because  that is more  than enough.  

Thank  you, Scott.  

The main thing I wanted to [  Indiscernible ] was that indeed  there was a great  deal of thought that was put into  the design of the software and if  you are at all a programmer  like me you probably have committed  many evil acts of programming  where you build lots of software  that probably doesn't deserve  the blame -- deserve the name software  and  was organized. The work was very  well done in this case. This  gets one -- gets to one of my favorite  parts of the model and similar to  the class structure on the see  sharp code. Basically as I said earlier a couple  of times in addition to the data  repository role that was part of  the role of trans CAD in  this model we also use Microsoft  SQL Server  which is very powerful what they  call enterprise scale relational  database engine and some of you  have worked with this before yourselves.  And so we have put quite a lot  of part -- thought in the way we  would design the table structure.  A lot of the stuff turned out very  simple and quite elegant where  you are -- you got  household that [ Indiscernible ]  and persons to foreign key to household  meaning that the persons or each  one is attached to another household.  And these good on the chain. Each  person  starts developing trip information  as we run all these components that  we've  been discussing. We start filling  up tables that describe and hold  the  travel behavior that model has shown  that these people  are exhibiting. So we start out  with a tour table attached to each  person and they have one  or more of these and as a matter  of structural convenience we also  have half -- a  half tour is from your home to the  primary destination is  one half to her and  the other. They're all  closed tours. And so each tour has  to  half tours.  Each half to or has stopped Sir  Tom  mentioned earlier that we developed  intermediate stops and intermediate stops are then  assigned to the half tours through  the models Tom described. At  that point we have basically set  up the complete depiction of all  the points that  any tour is traversing. As Scott mentioned  earlier some components that aren't  really model components. This is  a place where one of those comes  in that all we have to do at this  point is start spitting trip records  into the  trips table so there is a little  component that does that just by  reading the have to were  stops table. And having done that we didn't  have a trips table with no characteristics  other than origin  and destination. And then the trips  model run into all the things that  Tom described about providing mode  in time  of day. It wears out very easily  and it's easy to explain and easy  to core area and get data out. We  also have a  similar structure of the  built environment. Tom mentioned  we have individual point data for  the households and all of the jobs  in the model and so you can see  how that is stored here.  We  have an onto this points table you  might say in the middle and then  we have a  bunch of different types of  points so subclasses to use in this  object-oriented language to describe  this. We had schools that were points  and houses and jobs and several  other kinds that you  see here. And then finally all of  those points have a zone ID so they  are all themselves foreign keyed  to the zonal table so we know what  zone they are him. This has proved  to be a simple and easy to  use design for storing the built  environment data  as well.  

I mentioned  the most [ Indiscernible ] this  is a sequential slide that has more  bullets as we go  along.  Finally as Tom mentioned earlier DRCOG did  all the calibrations and validation  work with some consulting from the  consultant team in the  classic sense where  we would call them up and say what  you think about this and they would  give us advice and we would  do it. That was  another thing that I very much recommended  you do as much of that is you can  because it's another of these steps  at which you really learn a lot  about the model and frankly you  learn  things like even if he didn't do  any work in building the model itself  up front you learn things like okay  where are the components and where  the constant terms. Where  are the  coefficients. Because in general you will have to play  with those a little bit as you go  through  calibration validation. You will  go through this very useful  learning exercise. You become very  intimate with how well the model  is turning out and you become a  lot more  comfortable with its results and  you feel like you know them better  and you're able to explain them  and justify them to others in a  lot more  knowledgeable fashion.  

This may be  -- calibration. I do want to emphasize. We been  working with this model for two  years and we worked or that tripped-based  model for maybe  40 years. So everything I say about how it  is turning out at this point I would  really emphasize is still pretty  darn preliminary. Having  said that the results even for forecast  years were in the base year of course  the results came out similar because  we were calibrating against the  same stuff at the aggregate level  and things like transit ridership  in BMT. We were calibrating the  same stuff we calibrated the old  model again so it would come  out similar. However the forecast  year outcomes were similar also.  I don't know if it will stay  that way as we refine this thing  as time goes on. But so far it kind  of has. The only thing that is been  different lately is we've been getting  a lot as I'm sure all of you have  is  lower Highway volumes to calibrate  against like  in 2010. So the outcomes are somewhat lower  than our initial calibrations because  we're calibrating against something  different now. We are doing a bunch of updating  your project I will talk about in  a little while where we are basically  going during exercise that I'm sure  many of you have  gone through in looking back and  forth with FDA in the  context up examining the possibility  of doing new start  applications and putting the thing  through the paces that FDA likes  to  do and the microscope that they  put your model under  so well. We have done some of that  work are ready and it's been enlightening  for us and I hope  for them. And we will be doing more  of that in upcoming work. The other  thing I want  to mention and this is an extremely  helpful item for us as we built  a spreadsheet  that using VBA back inquiry the  SQL Server databases so that as  we are running the model  we can in real-time run these  square ease  as the data tables are being filled  by the model as it is executing  and see how it's doing. And at the  end of that we have  a large body of results of each  of the components as we've gone  through and that has been easy  to write and has been  extremely useful as we have been  going through model runs along so  I hardly recommend some tool of  that sort. I think I have party  belabor the topic of the  slide so I won't address it. I think  everybody knows what  it says.  

We will take a break here and  see if there are questions that  people  would like to pose before we go  onto the last parts of  the presentation.  

Before area talks about the model  application work we wanted to give  folks an opportunity to ask questions  about the model itself  and development process he just  described. There is one that has  come in already so I will pose it  to Erik and Tom and  that is how where the candidate  point parcels  from the Monte Carlo simulation  updated for use in future scenarios?  

[ Laughter ]. I guess I better  take  that one. That relates to a couple  of things. Basically what we had  to do was  to generate a whole new set of points  for future years as  the questioner tumbled to. So what  we did was and what we are still  doing because  I telegraph the last slide here  a little bit by saying we're in  the middle of building  and UrbanSim in limitation because  we don't have  that now we are stuck with using  our trip-based -- our  old zone-based somewhat homegrown  land-use model. That  model generates as I'm sure many  if not all of you have similar ones  generate the jobs and households  within each traffic analysis zone.  So than the job is simply to generate  a point  or points that inside each zone  that match the number of household  a  job point that the land use model  has allocated to that zone. I will  always like to  say that and I said this a lot early on  when people would say to me you  are crazy and trying  to put -- generate an individual  asked why point on a map for every  household in every job. And I  would say we already do that in  the trip-based model and that I  would pause for effect is I'm doing  now. And then I  would say that point in each traffic  zone is a centroid. All the points  in the zone get the same -- all  the jobs and  the households get the same point.  And then I say although we know  that the approach we're taking  to allocating the household and  jobs to individual points  is approximate we can beat the idea of getting  them all the same point could easily.  That's a low bar to  get over. So the other elaboration I will  provide as we started with a parcels  that because we had a complete parcels  that.  And then we basically do at  one point my [ Indiscernible ] by  we build this thing that we call  babies them and ripping off of UrbanSim  and we called it  points them and models as serious  as  we know. We start off with a parcels that  and then we do a very approximate  scoring of  the parcels and I could go on at  great length about that that we  scored  the parcels and we take a shot at  how much capacity each parcel has  base of  the zoning and then we distribute  the points  based on the type the development  that are permitted H partial and  how much capacity the  parcel has. And that is how we do  it in  a nutshell.  

And you are saying this is easier  than just developing or year 2040  parcel database  right now?  

I don't know if [  Indiscernible ] and if he is the world to me say  that my understanding is that they  and possibly other people that I  don't know about are working on  a purchase for example to starting  with a parcels that and subdividing  the parcels into the future. We  don't do that. We just for example  if we have a parcel that happens  to  be  large and we have put up household  inside that parcels we will randomly  distribute them inside  the parcel. There is a tool that  allows you to  do that. It is not the cleverest  way of doing it but it beats the  heck out of putting all the points  at one centroid in the middle of  the zone.  

And so there is a follow-up question  that is been submitted. How do you  deal with result variations due  to  the money -- Monte Carlo simulation?  

The short answer is right now  we don't. We have  run it -- other than to say that  anecdotally we run the model quite  a number of times and it seems pretty  darn stable. It really doesn't give  you a lot of difference  in outcome at any level of aggregation  that anybody particularly wants  to  look at. When for example we run  the model over and over again and  look at  the differences in let's say a transit  ridership by seven mode, rail versus  bus, a gives us the same answer  every time. The only follow-up comment  I would get to that is that and  this is an interesting topic for  anybody who is doing highly geographically  aggregate and demo GUI -- demographically  aggregate modeling -- it produces  in  our case data that is fully disaggregate.  I usually save for another allegedly  humorous effect I will say well  my house in  South Denver is in this model and the model  puts a family or somebody into that  house. I could go  into that database record and see  who is there and I would bet you  anything that it is not  my family. But we are not going  to show anybody data at  that level. So I am certain that at that level  if we run the model over we will  get a different answer every time.  But nobody wants to look at that  or if they do we don't want to  show them.  >> I don't see any other questions  that have come in so I think we  could -- here comes  another one. The question is can you elaborate  on what your DBA enabled spreadsheet  where he is while the model is running  and what the goals of that effort  are.  

 For example if we looked at that  -- those flow diagrams of the box  and arrow diagrams that show the  sequence of models so and  this is near and dear to my heart  because we got done running all  these models for regional transportation  plan a date this fall and I spent  hours on the weekends and evenings  doing  that. Especially as I have not mentioned and I think  I will mention later so I'm telegraphing  again, one of the things we've been  happy about is that we  sadly lost the last of  the very core developers of this  model except for myself to a competitor  who will  remain nameless. And we were still  able to run the  model successfully with the staff that we replaced  it with and I consider that to be  a great validation of our approach.  The first Hummer brand to the model  without her it was not the easiest  thing  to do. So we were trying to take  it slow and careful. So what this  spreadsheet does  which is [ Indiscernible ] and  Childress wrote is it has a different  tabs like any spreadsheet and then  each one of them has  a query using DBA that you  can do some sort  of ODT connection to the SQL Server  database. It will run a query off  the database and field data into  that tab. Let's say we're running  the model  sequentially and we start out with  the workplace location choice model  which I think is the  first one. Once the workplace location  choice model  is done and by the way the code  is set up so we have a little  text file and at the beginning of  each component a line of text is  written into the text file that  says "opponent and get the time  and when it's done a gives finish  the component and gives the time.  We know when it is done and at that  point we  would run the database query for  this particular tab and it would  spit out some summarized resulting  in this case we summarized by  20 districts to see what the  distribution was of the  location choices that the workers  made. We have a set of  observed data that is an updated  by the square a but stays there  all the time and the two of those  are columns of data that's  it there and we have some statistics  that automatically are prepared  inside the spreadsheet that anyone  can do -- percentage  difference between the model results  versus the observed for a given  district. Right away we  can see the outcome looking okay  or not. That helps  us to watch the model as it is  going through  in real-time and this model is still -- we have  an optimized it by any means in  terms of runtime but it takes about  24 hours. It's nice to be able to  tell if you are doing okay or whether  you are to stop and fix  a bug.  

So it is  a QA  QC tool. The questionnaire hinted  at in their question.  

It is and  I also a record-keeping tool because  at the end of the run you have a  summary that shows how the thing  came out  in detail.  >> If the ABM and the land-use model  are  run sequentially for horizon years  with feedback between the two models.  

Since we're not  done yet it will run  that way. We're not  done yet. I have been having some  conversations with the staff that  urban analytics about how that will  be done. But for those of you who  are familiar way that UrbanSim is  supposed to run we feed it basic  and for but -- if the  data like logsums and  other variables from the base year  and UrbanSim runs at several years  -- typically five in sequence and  then we have the  base  was 2015 -- sorry, 2010 and we have  a 2015 year model rented we don't  have any  of between so we will run five years  of UrbanSim  and pause and run the travel model  and spit out more results  to UrbanSim more accessibility variables  and continue on. The old-style  land-use model does not work that  way. It is a much simpler and less  accurate elegant item. It forecast  straight out to forecast years such  as 2035 and we interpolate backwards  to generate the intermediate year  land-use data sets of the those  are  fed statically into the travel model  so that's how it works  at present. All of the things that  are bad about that are why we're  trying to build a borough --  urban implementation.  >> Software  licensing agreements. Is focused  considered open  source or closed.  

It is not yet. I put my manager  is happy to  do that and we have been so busy  that we have not been able to get  through the legal side  of it. If anybody  is interested in getting a hold  of it I encourage you to call  me and I have no doubt that the  management here will be happy to  figure out how to  accommodate you.  

Thank you for those questions  everyone and we can move on to the  second part of  the presentation.  

This is the part where everybody  says Erik is talking more? But I  think Tom and Scott are done but  you just  give me.  >> This is where we talk  about applications. I believe federal  highways was expecting and hoping  people would be interested in how  we have been using the model since  we developed it. I do  want to digress briefly and  say that a lot of what has happened  as  people have talked back and forth  about whether it was a good idea  to build models like this in the  first place, and this was the same question  I was asking at that same stage  in my development on  this work so I don't pretend to  be superior  to anyone . They would say I wonder if this  gets better answers. Doesn't get  it -- give you better transit ridership  and so forth. The answer is  well maybe. Is certainly no worse.  But the key to me is  not that. The old models weekend  spent a lot of time trying to get  them to produce his numbers and  you produce them well. What these  models do is a bunch of  other stuff that those older models  do not produce at all and as I  said earlier in  this presentation, I have dog tired  of telling people I couldn't produces  a now  we can.  Amusingly enough -- we had been given a fierce deadline  of August 2010 to get a working  version of this  model going after diddling around  with it for  several years. So we were working  frantically to get  it done by the deadline so we could  do our transportation plan update  that fall. The  same customers our transportation  planning post  and say by the way could you help us do  some selection of bicycle and pedestrian  projects for our transportation  improvement program  for funding. Our first thought was  were not done  yet but we could  produce something. I have to say  that at that moment we had no ideal  what we would  produce would pass anyone's test  of all because it'd never been used  for anything in any way to try to  come up with  an answer -- statistics that could  to  support decision-making. It within  an interesting  exercise. Basically it turned out to be a rousing success  of this was the first time out of  the box for  the model. What they wanted to do  was to try to come up with some  way of developing  some measurements of desirability  of different bicycle and  pedestrian projects. What we ended  up saying was  how about if we do things like calculate  how many  trip ends and origins or destinations  there are a bicycle and pedestrian  trips that is these trips that a  been selected through that  choice model and those a separate  by  the way. We could count inside of a bucket  around  the project that had been proposed  and was  requesting funding. How many origins  and destinations of bicycle pedestrians  trips there were. We can calculate  statistics about the strips such  as with the average life of them  were. So we  did that. It's worthwhile noting  that and going back  to that  diagram of database tables that  I  discussed earlier. If we had tier 2 we could've provided  them detailed demographic data about  every person who made these trips  because all of these things are  connected together. The trips are  key to the two words  which are keyed to the people which  are key to  the households. We didn't go that far because we  figured that was crazy for the first  time but to note that you can get  information like that whether you  should or not is  another matter.  We were figuring we were on  safe ground. It turned out that  the numbers  were reasonable and I wanted to  provide one graphic. I apologize  it did make our graphics people  make the dots bigger and I'm hoping  you have a bigger look at it than  I have.  The green  crescent moon in  the middle is the proposed project.  This is a pedestrian overpass over  I 25 and Southeast Denver area.  And basically they have decided  we will do a half-mile buffer around  that thing  for pedestrian origin and destination  than a mile and a half or  bicycle origins. You can see that  the rust colored points are  the pedestrian origins  and destinations and they --  the teal colored ones of the bicycle  ones. Those are  precise locations I do want to emphasize -- those  are land-use locations. Household  jobs and to which or from which  the bicycle or pedestrian trips  were either coming or going.  Those are tied into the land-use  pattern that we did. We don't have  time to go over  the details and I did it two years  ago so I forgot some  of them. The results of the differences  between one project and another  looked reasonable and our transportation  planning people went away happy.  We felt excited about that and I  still do.  

Another  similar analysis. As part of our venture vision plan  we got a bunch of areas that we  are trying  to institute higher density development  and create these urban centers around  the region. The plan involves trying  to steer more development into  these things and many of them are  along existing or planned rapid  transit  lines with the usual reasons that  I'm sure I will need  to explain.  Enhanced transit ridership and so  forth. One of the things we were  asked to do was  to calculate how are these guys  are doing. Was for the development  to they have in them and what  travel behavior are the trips and  people associated with  them exhibiting. This is another  one of these things where in the  past I would've said look at this  funny shapes and they  don't really conform to the zone  boundaries that  we have and so I'm not going to  be able to give you accurate answers  because I will be giving you  numbers that are aggregates of results  to and from the zones that roughly  approximate  the shapes. Then I would have said and I also  can't give you anything about bicycle  and pedestrian because we don't  have that. Now I didn't have to  say any of those things. Now because  the development is depicted at the  point level the jobs and households  that individual points I  can summarize any statistics that  the model provides in any geography  that anybody asks  me for. There is always impression  of whether these numbers will make  sense again and that was another  one of these I am sort of my teeth  are chattering when people are asking  me can you give me let's say  the most share of people who work  in the zones.  How much SOP goes to each one  of these. This is what a  customer asked. Does this person  hope to be able to judge which ones  of these are better and worse according  to that statistic of  which ones are doing a better job  of getting people out of  SOP's. Again since the development is that the  individual point level and the trips  attached to those individual pointed  as I  said before the trips tied to the  Tories which tied to the people  which tied to  the households we can provide any  statistic that the model is capable  of producing for any of these geographies  and that is what we did. It was  one of these  drumroll please and can I have the  envelope please for things that  we look at the results and I'm not  expecting you to  read this but this is about half  of those urban centers in each one  of  these records. We generated a whole  bunch of statistics and one of them  I mentioned. SOP share and we generated  average AAA and we generated crazy  stuff like what percentage of trips  that start inside one of these  urban centers stay inside the  urban center. You  can manage internal capture statistics.  Once again what was most interesting  to  us was are these numbers going to  make sense. I will spare you  the details. Basically the result  looked very reasonable. I  was shocked and have come to be really surprised  at how reasonable looking the numbers  look at what  small geographies. So that is been  another thing I have been  thrilled about in the latest models.  This is another under --  unreadable slide. This was here  for  a fact.  

To convince you we generated  a bunch  of numbers.  >> This slide is the basic items that  I have talked about. We  did travel mall runs using  the systems we talked about for  basic year and forecast year up  to 25 and the model produces all  the usual outfits  from GISDK that the old models produce  such as the MTA  transit ridership and all of that  stuff I've been talking about from  the SQL  Server database. And the results  on the BMT side of an similar and  likewise on the transit  ridership and we have just completed  a second round of running  the model and the same thing is  happening in  this regard. With updated the regional  transportation plan for 2012.  I expect DRCOG frankly  to be updating this model or a similar  one after I'm dead  and considering we were still updating  our old model after we've been using  it for 40 years is an excellent  chance of that. And how it will  play out as the years  go by I wouldn't pretend to be able  to tell you. But I will say is that  model  at present is producing plausible results  but that the usual older scales  and at scales that have surprised  me a  great deal.  

Another project is Colfax  Avenue alternatives and if the old  U.S. 40 which is the giant east-west  arterial or town and aggressive  the middle of town and it's an urban  core door. Lots of commercial and  retail space. Very tight  space constraints. It goes to the  urban heart and is built up and  has a lot of  transit use and all sorts  of things. It is a first project  where we are using the bogus model  for outside of the  DRCOG normal process. We  felt that the model was  essentially born to support projects  like this. Some of the things --  there is a ton of nonmotorized use  in the quarter were door -- core  door  and development. And there's a lot  of interaction between motorized  modes of the  nonmotorized modes. There are a  lot of high density residential  areas along this corridor or and  people walking in  and out to access the development  along the core door to get on transit.  It's the sort  of problem that models like this  we sure hope are  going to help us to solve better  than in  the past. There is a consultant  who is working  with us to install this model outside  DRCOG for the first time and  run it so we are pounding away doing  that and are in the middle of it  and will be going to this  new start put it through its paces  thing I  mentioned earlier with federal transit  administration and that will generate  interesting conversations.  

Maintenance comments. I mentioned  we lost all the key development  -- developers  except me. The consultants provided  all the  logit codes and data sets a we have  been able to reestimate things.  That is been  extremely helpful. We are in continuous  update mode and I mentioned that  the DA Accel  SQL  Server tools. The things I like  about what we  have done. What started out being  referred to as DaySim theoretical  structure has been were 12 --  working well. As Tom said that models  are branching  and branching and I expect someday  they will come back together again  a bit as model is InterBase models that  over the decades but at the moment  we are in branch mode as near as  I can tell.  

I  hardly recommend strongly enough  that you use a relational database.  There different ways to tie that  into  your model and I will talk about  that right now. It's a  software discussion that probably  you don't want to get into now especially  since I'm losing  my voice. Whether you have a tightly integrated  relationship database to your model  during operations or whether it  is losing you do  things like spitting data out into  a text file and then exporting into  the database or another approach  I recommend you use a relational  database and design  your data to make it easy to pull  it into  at minimum that structure. It is  incredibly easy to work with. And  you can generate sophisticated piles  of summarize data with very  little effort.  

Land-use -- read the labor that  a lot. I have party noted the  geographic flexibility  it entails. It ties naturally  into UrbanSim's and UrbanSim's picks  out stuff in the same type  of format. We have talked about  its use  of nonmotorized modes. One of the  things we talked about at the beginning  of this project is we wanted  to use -- I will digress into an acronym  discussion for a moment because  what  the heck. What are the things that  was interesting with the travel modelers of the  software people started working  together. People like Scott are  like regular genuine software people  and we spoke  different languages of course even  though we essentially speak English  together. Everybody had their own  acronyms. I got to learn how to  say Cox -- CMTS which downs for  [ Indiscernible ]  off shall. We made this decision  that we would try to use as much  of that as we could in this model  on the theory that there would be  people who  were just attracted to our model  world who could help us with  the model and that is actually prove  to be the case in some of the work  we've been  doing recently. We have been engaging  consultants in helping us optimize  the runtime and they don't know  anything about travel modeling but  a lot about SQL Server  database and C sharp applications.  So use of  that technology has so far been  proving itself affected so we been  happy with that decision.  

We  have developed a boatload of documentation  for this thing and that is been  helpful  to us. In being able to run the  model under  present circumstances. When I started to think about it  that when I started at DRCOG and  no one we had 1000 pages  of documentation on the old model  so I guess we haven't done  anything new in this respect that  we do have a lot of reports on this  thing of all kinds that  have been very useful to us and  we continue to generate more as  time goes on.  

I would make the database more  simple and not  so much the part of it -- if I had  this to do over again not so much  the part I should you but the part  that controls the components of  the model. I will -- it was more  complicated than we needed  to be. We got fancy on  the plumbing structure that we employed  in building the parts of the model  that pass data around a controlled  component execution. I would simplify  that down more if I had it to do  over again. I estimate a few more  models in house because it is fun  and it  turned out we were not getting in  people's way too much and we are  not stressing  our capabilities. I probably formed  partnerships to give up the model  afterwards a sinner. And try to  work on that harder although what  happens is when you  succeeded something like this model  then the next thing that happens  is they give you a bunch of other  similar projects  to do. We got a couple of other  next generation models piled on  our backs and are having a harder  time pursuing some of those partnership  options which  take effort.  

 We're doing UrbanSim now and scheduled  to generate a first usable draft  in August of 2013. This keeps me  up at night  a lot. We took the liberty of University  of Arizona -- there is  an interest going around the region  encore  door studies and making use of that.  I was in a meeting about  that yesterday. Commented that we are -- have the  money to start trying to look into  building what we're referring to  as a complete network set and try  to integrate our map of bike facilities  into an actual network and try to  see how much progress we  can make on  pedestrian facilities. As a mentioned are ready we will  be continuous upgrade mode on these  focus models for the foreseeable  future and that is  all fun. I think that is it except  to say that if you want some more  -- if you want to read some of  these documents -- the team put  a bunch of them  on their and will keep you amused.  The link is there at the bottom  of the page. And with that I am  done except for answering questions  that I  hope you can ask a top questions  because I'm about out  of voice.  >> I am going to open up a short  evaluation pull --  pool that participant should fill  out quickly. About 20 or  30 seconds. A while folks are working  to that we can address some of the  questions that came in during  the second phase of the presentation.  There is one asking what nonmotorized  access abilities to use in the Mode  Choice model without  an explicit  bike network?  >> What we  have is eight  to set of access  abilities or -- not accept abilities  but impedances. In the Mode  Choice models the nonmotorized modes  mainly use a distance as the impedance  rather  than time. Essentially you don't  have information on how fast people  walk and bike and in fact the variation  and how fast people walk and bike  is more than how fast  they drive. We just use a distance  which is something  that was easy to compute. As we  talked about before the  nonmotorized trips are very short  so zone to zone times are going  to be  highly inaccurate. So for the really  short trips will be used is  something -- that was orthogonal  distance meaning  the East-West distance plus the  north-south distance between the  two points and again as  we had the points identified for  this  specific locations computing that  distance is  pretty easy. For the long trips  we basically would use of things  from the zone to  zone network and their trips  that are in between long and short  that we would use a combination  of the  two. So if it was a little bit --  I can't remember  the exact limits for what was a  short  trip but a medium  length trip. But if it's in the  medium length it uses a combination  of the orthogonal distance and  the skim disincentive is closer  to being a short trip and it is  more heavily weighted  towards the orthogonal distance  of the closer to being the longer  trip is weighted towards a longer  distance.  

 That's right.  

See, I have a memory.  

I did notice by the  way that there was a question before  that one that I don't think we answered  about runtimes.  

 Someone asked for you to speak to the computer  hardware being used and what kind  of runtimes you  are observing.  

We have done very little to optimize  this thing and are starting to put  our toe on the water. It takes 24  hours to run the bottle and we're  running on 32 core server with 64  gigs  of RAM. There are a few things we've learned  about optimizing and I will share  one of them  right now. This is [  Indiscernible ] that at least with the way our  application is build to turn off  the virus protector to the server  it  runs faster. And then basically  I'm not an expert in the  virus protectors but I understand  that they examine files with those  files are accessed by an application  to one level -- 1&#;  or another. There is a bit of time  taken whenever you get a file  for use that the virus protector  depending on the settings  you select the virus protector will take time  examining that file to make sure  it doesn't have a virus. Since  our server is way behind  firewalls and hardly anyone else  has access we feel comfortable.  I have to say that I do  feel that I would lay a good amount of money  that within two years of this thing  will run at half the speed it is  now.  It's not less than that. Some of  the things that we are thinking  of doing are  hardware related and what consultant we've been  working with suggests we might get  huge improvements by using solid-state  hard drives and we will examine  that possibility. I was talking  to software people the  other day and this speaks to the  subject of object-oriented design  and  code reuse. The people I was talking  to the other day said that they  have been playing with  Monte  Carlo algorithms for selecting when  you have a set of  probabilities that a logit model  spits out a doing a random generator  and selecting the alternatives.  They had been originally done  algorithms but that are similar  to the one we use and have concluded  that algorithm is slow and they  came up with the one that was a  good deal faster and  of course the beauty of that a better  that to be true that Monte  Carlo algorithm is used and how  many times and many many millions  of times in this model so if  we get savings out of that it will  make a  big difference.  

 Another question. Do you have a sense of what or  anything you need to do to make  Focus accessible to FDA.  

 We do and we've had some early discussions.  I will give you an example. And  I will -- I was a few words and  ask it off to  chip in. --, To  check in. When one of the main things  you are trying to do  is support the user benefits calculations  that summit produces an  summit takes outputs from the Mode  Choice model and now we have two  stages of Mode Choice models of  course where the old trip-based  models only  have one and said there was discussion  about which of these stages should  it be two or Mode Choice or  triple combination. What should we do. And where should  the take copy to spit out results  to summit. In interest of not trying  to put FDA in a box I won't  tell you how that conversation is  going. But we did come to some conclusions  after some discussion about  that about how we thought that would  best  go and I might be able to say it  anyway but just  in case FDA is feeling like we're  in the middle of the conversation  and I won't but we  came  reasonably -- a meeting of the minds about how  that would work. That is one example  of things that we will be looking  at. The other things I think that  anybody is  working with or another example.  Things that anybody who is worked  with FDA is familiar with. They  are very concerned about consistency  of the model  and so value of time and how it  is depicted in  the model. That will -- these models generate  more conversation on the topic because  we have a lot more purposes. The  old model  we had -- so-called purposes you  can tell what  Assurity activity person I become  and they call them so-called purposes  but home based on work and non-home-based  we have seven  purposes and work and shopping at  school and escort. Social  recreation etc.  

But not home-based. Think that  is for that. So all of them  had different values  of time. And you quickly get into  discussions about because one of  the things FDA is interested in  and again rightly  is comparing the value time you're  using when you were doing our Alyssa  what you to use in your Mode Choice  model. If you have seven different  purposes and different values  of timing are you going to do seven  different sets of scams in your  skimming section. So  you can be the consistent paths?  You can quickly get way  too many [ Indiscernible ] doing that. There  are discussions like that that have  yet to be resolved. I think we will  all get there and it will  be good.  >> I was going to say the only thing  I would add is of course with MAP-21  the FDA requirements are going to  be changing anyway.  So obviously we are waiting to see  what comes out of that FDA hasn't  had much of a chance to react to  it yet. They are working on it so  I'm sure there will be more on this  to come.  

Good point, Tom. I don't see  more questions and  we are closing in on 4:00 so that  will conclude today's webinar. I  want to  thank Erik and Tom and Scott for  taking the time to prepare and deliver  this webinar and for sharing their  experience with DRCOG. If you have  questions about  the content the turtle that was  presented today send us an e-mail.  The address is feedback at [  Indiscernible ] TMIP.org. I want  to announce that  our January webinar session will  showcase the advanced freight  forecasting work that is being done  at the Chicago Metropolitan agency  for planning and that is scheduled  for Thursday, January 31 in the  same timeslot. So thank you again  Erick and Tom that biggie to everyone  and have a happy holiday season.  

 Thank you.  >> [ Event  Concluded ] 
