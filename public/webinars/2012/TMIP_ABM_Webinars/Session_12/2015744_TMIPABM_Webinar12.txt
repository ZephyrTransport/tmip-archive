<meta name="viewport" content="width=device-width; initial-scale=1.0; "\>Event ID: 2015744
Event Started: 9/20/2012 6:00:00 PM
----------

 9/20/12 [ Captioner present.  Please stand by for real time.  ] 
>> Okay.  I think we are just approaching time. I'm ready to get started. Welcome, everyone to the TMIP webinar series. Today is the 12th in the series of 12. We are at the final presentation. Today's seminar will be on forecasting and application. So how these models actually go into use. I'm Steven LAU and I'm from RSG and moderating this session. We have two speakers today. John Gliebe and Peter Vovsha speaking second. 

>>> Today the seminar will go for two and a half hours. And we'll address questions halfway through. So if you have any questions, please use the web portal to post those questions and I will look through them, organize them and ask the presenters these questions. If you would take a moment and just the first minute or two of the presentation, there will be a poll available and we hope you take a moment and fill that out. And at the end there will be another poll and we hope you take a moment and fill that out too. Very useful for this webinar and future webinars. So, with that in mind, John Gliebe I'll turn it over to you. 
>> Okay. Thank you very much, Steven. First of all, I'd like to give some acknowledgements. This entire series has been a collective effort between RSG and Parsons BRINCKERHOFF. John Gliebe, myself and Peter Vovsha are your presenters for today and we developed a lot of the content. I should mention that MAREN outwater prepared most of this presentation. Mark Bradley reviewed the presentation. And multi media producer is BHARGAVA SANA. We have similar teams that have contributed to all of the webinars throughout the series. 

>>> Okay. So if we look at where we're at. We're now here at the final part of the series forecasting performance measures and software. This is the 12th part in the series. And I would just like to put this up and invite you, if you've missed any of the sessions in the series, they are all archived on the web. And you can download them and view and listen to them at that point. 

>>> All right. So as we mentioned today, we're talking about forecasting, predominantly. And application of activity based models. 

>>> At the end of this session, we're hoping that participants can answer basically the following types of questions. For example, what are the steps involved in preparing activity based modeling systems for forecasting. What output measures are produced with activity based modeling system? What are the differences when you are performing an altertives analysis or similar types of applied work? And what are hardware and software considerations for web based applications? 

>>> Okay. So before going further, it is worth revisiting where we've been, I think. This being the final webinar in the series. We have now covered all of the fundamental components of activity based modeling systems. And this includes the creation of synthetic populations and important model inputs such as the treatment of space and accessibility. Long-term and mobility choices, tour and trip details such as scheduling and time of day choices and tour and trip destination and mode choices. And network assignment and integration with the demand portion of the activity based modeling system. 

>>> We wanted to show you this slide again. You'll notice there's downward integrity on the left and upward integrity on the right. And throughout this series we've emphasized this how the system works as a whole. Again, downward integrity: Choices made in higher models. Whereas upward integrity is where expected utility of the down stream choices affects the probabilities of the choices made before them in the upstream models. 

>>> So now we're ready to consider these model outputs. This red one here in more detail. Here's the outline for the remainder of the session. First we'll talk about the importance of forecasting methodology, performance measurement and software development. We will define some basic terminology. We'll spend a bit of time on calibration with activity based models and what you can expect from that. We'll look at performance measures, discuss sensitivity testing and discuss how random variation may play a role in alternatives analysis. And in the second part of the webinar, Peter is going to give you a whole bunch of interesting applications of activity based models and he's going to discuss implementation in hardware and software and areas of future research I think you may find quite interesting. 

>>> What are the differences in activity based modeling in forecasting practice? One important difference is there is a much richer array of output measures that are possible. The second important difference is that the internal complexity of activity based models requires new understanding of how to properly calibrate and validate models. This is because there are just more model components and there are a larger number of inter dependencies between the modules. And in addition, it goes without saying, but I'll say it anyway. Activity based models are not just about trips. There's a need to interpret daily patterns and tours. And activity durations are important to comprehensible forecasting. In addition, the use of simulation to produce forecast does make it necessary to control and explain random variation through the simulation. So of course, this is important if you were to producing consistent forecast and communicating with decision makers. 

>>> And finally, because of this added complexity of activity based modeling, model systems, I should say. Means that new application software is needed. And this is for two reasons. One, to take advantage of the more powerful analytical capabilities. But also because we're asking our computers to do a lot more. And so the software needs to be designed very carefully for that. 

>>> Let's look at terminology we're going to use during this session. Some of these terms you have seen before and some of them you have seen in other context. So this slide defines some of the key terms as they are used in this presentation and throughout the webinar series. The first one is micro simulation which refers to a travel demand model that simulates individual agents either persons or households or vehicles. So this is different than just pure traffic micro simulation. Here we're using it in a more general way that simulates individual households and people. Performance measures in this context would be outputs of the travel demand model that assess the benefits of a tragedy oral turntive. For example, you are commonly used to seeing vehicle miles traveled or travel time savings. But with activity based models there are a whole lot more. Forecasting here we're talking about representation of a future year scenario along with all of the assumptions that under lie that forecast including input assumptions about growth, transportation, infrastructure and the economy. We're going to talk quite a bit about data visualization towards the end of today's session. And in this case we're going to be looking at graphic, tabular or spatial representation of model outputs or inputs actually. At the end of the session we're going to talk about research topics. One being high performance computing. We've listed these two terms here. Multi threading is where we're processing our program, our data across multiple cores within a single computer. Whereas distributed processing in this session is processing across several computers in a network. 

>>> So let's get into the steps involved in forecasting activity based model. In many ways, they are very similar or analogous to those in the trip based model. One starts with base year calibration and validation. And so it's typical to create horizon baseline forecast. When an agency first develops an activity based model, it's common to compare the forecasting results to those of their trip based model. That's a common practice when an agency is getting started. This helps the agency to spot differences between the two and hopefully explain those differences. Some general sensitivity testing is also recommended. So this would mean carrying certain input assumptions. To make sure that the model system response is expected. We'll talk a lot more about that in this presentation. So once an activity based model is put into operation, there are potentially many uses for it. We've listed a few. We'll talk about several in this presentation. Starting out, an agency may want to try it out with basic types of alternative analysis such as network alternatives. Changes to the net WOSHLG, coding and facilities. Land use alternatives and potentially other policy alternatives. And it is important for an agency adopting a new tool to become comfortable with a way it behaves. To do any necessary fine tuning and know what to expect when it comes time they need to do a long plan range update or some type of policy study. 

>>> So focusing obeys year calibration and validation -- on base year calibration and validation. Kind of a two face thing. So first, there's the individual model components that are calibrated while holding other components constant. Typically, you would be estimated from a household diary survey. And it would be expanded and calibration target values set for all of the model system components. You might supplement that with the household survey data with survey data from other sources such as the census journey to work data. That being perhaps a more comprehensive indicator of commuting trip lengths and spatial patterns. Another consideration, the activity based model system will have many more model components than a four step model. So there will actually be more points of calibration. And this adds to the amount of time required for calibrating the model just because there are more things to calibrate. But the idea is that it allows the model system to respond in a more behaviorally consistent way when you end up applying it to various scenarios. Especially as they happen to deviate from baseline scenario. 

>>> So then the next phase is system level. And here is where we want to make sure that the individual model components remain in calibration. So now we've looked at each thing individually. And now we're going to run the models as system with feedback and check our calibration target values and probably make some minor adjustments. In general, our experience, we do not expect an activity based model to do a better job of matching traffic counts or boarding counts than a trip base model in the baseline scenario. If the two models are started from scratch with no calibration, the activity base model generally gives a more reasonably first pass result. Usually it's the case, the comparison is to an existing trip base model that's already been calibrated. So typically easier to calibrate an activity based model and avoid the use of extensive K factors you might otherwise have in a trip base model in order to get that to cam BRAT 

>>> This diagram represents that second phase of calibration where you are starting at the top at 12:00 at model inputs and generating synthetic population and calibrating these long-term choice models. We talked about work location and school location in our session on long-term choice models. I believe that was session five. Should check the schedule. And we also talked about mobility choices such as auto ownership and transit passes. Ownership models and others we could have. Then we would go to calibrating daily activity pattern models. Daily patterns, frequencies of tours and stops on tours and by purpose. As well as scheduling. And we'd get into the tour and trip details. The tour and trip destination and mode choices. And finally, go through assignment. And if we're doing this, you know, in a system wide fashion, we'd be looking at model outputs comparing to the observed counts as well as the calibration target values. Determining whether or not the forecasts were reasonable. And probably making a number of adjustments before we're satisfied we have a good model going here. 

>>> So let's look at some traditional calibration metrics. So these are all things you would probably be familiar with if you are a trip base modeler. Things like vehicle available by zone or district. Work commute flows by district. Activity and trip frequencies by purposes. Activity and trip frequencies by time of day. Trip length distributions and trip mode shares. So these are all should be quite familiar to most of you. In addition, there are a number of other things we want to look at that are specific to tour concepts and activity based concepts. One is activity duration. So now remember in activity based model we're not just interested in the trip that comes out of the activity. The activities themselves have intrinsic value to the people who are doing people. And they have a duration and this duration takes up space in the day. It's part of a time budget. Important to account for the amount of time people spend in activities and to have a realistic model that predicts that. There's a number of tours by type. Number of work base subtours. That's people going out to lunch or running errands during workday and returning to work. Number of stops per tour. Tour mode shares. We remember from some of our previous webinars we talked about choosing a tour mode and contingent upon that a trip mode. Auto tour lengths for the length of the entire tour, the distance. And transit tour lengths. 

>>> So one of the questions that often comes up in talking to agencies that are considering maybe moving toward an activity based model is okay it seems like these models are a little more complex and how much time is it going to take to calibrate them? Indeed, activity based models do have many more components and do require more time to calibrate just because as I mentioned, the individual components and then the system level calibration. In practice, what we've learned is that because you are calibrating so many individual components, you really have more support points for doing forecasts. And you have a disaggregate population. Remember we have a synthetic population. And we have linkages between these components. The investment in all of these linkages and aggregate representation of decision makers, decision points, means that there's a final -- excuse me. A finer resolution. Tends to payoff by putting in a better starting position. Leads to fewer problems when it becomes time to do the validation. And also leads to more sensible results when you start doing scenario analysis and different types of sensitivity tests. 

>>> So another issue that comes up is the notion of transferability of model systems. And I guess it's safe to say this is becoming increasingly common. At the same time, I guess the jury is still out whether or not this is visible. -- visible. -- ADVISIBLE. And actually transferred to the Tahoe planning agency. And that seemed to work well for them. There are other examples since then where the Atlanta regional commission and the metropolitan transportation commission which is the MPO, the San Francisco Bay area, sort of pitched in to CO develop a model system structure which the model system structure works well for them. Of course, they have to develop co-efficients independently more or less just because not everything is quite so transferable. Right now there's work in progress to transfer activity based model and get it up and running. This model is being transferred from the PUGET sound regional Council in Seattle. And the long-term plan is reestimate the model system once DVRPC has their note. The issue of transferability is of keen interest to federal highway administration. There is a project where comparisons are being made that have been developed for Sacramento and Shasta county in Fresno California and Jacksonville and Tampa Florida regions. That's federal research going on. 

>>> If we were to compare the model forecasting versus a four step model, it's safe to say this is enhanced explantery power in the -- explanatory power in the model. For example, consider tolling and a trip based model might not account for a one way toll because it does not affect the AMP skims in the particular trip based model may be that the trip distribution model is only based on the AM peak. Activity based model might consider going in both directions. So it would predict a more appropriate response and consider them at both times of day. 

>>> Another example would be the ability to test travel demand management policies such as short workdays and shortened work weeks which you can do with activity based model. 

>>> In general, because they are disaggregate representations of travel behavior. Give the analyst the ability to trace travel behavior outcomes back to the course of their change. Of course, one needs to know where to look. That's where the training and other experience come in. And in addition, activity based model facilitate better communication between modelers and planners because they can point to more intuitive explanations for changes in travel behavior. 

>>> Let's start looking at some performance measures that would come out of activity based modeling system. 

>>> We've broken it down here to mobility and equity. 

>>> Traditional performance measures are still available including things like trip length distributions, mode shares and link base and travel times and cost. 

>>> In addition, activity based models where they seem to have a great advantages in predicting impacts on individuals within the population. And this is especially useful when analyzing equity considerations. 

>>> I want to give you an example. Here's one from Chicago. A travel time analysis. I'm hoping you can see the fine print. But if you can't I'll tell you what it says. So this is total daily travel time predicted by the recently developed Chicago regional activity based model. The data are summarized by household income group and in case you can't see it. The four income group levels are in the black type. The sort of main headers. And the black dot along the horizontal dash line. If you follow this, the total daily travel time per capita increases as you go up the income levels. In fact, there is nearly a 20 minute difference from the lowest group to the highest group. This shows large differences for daily travel minutes for different person types. And in this example, full and part-time workers have the highest values. Followed by university students, followed by driving age secondary school students. High schoolers.  And then nonworking adults, not driving aged students and retirees. And lastly pre-school aged children. 

>>> So activity based model gives you the ability to do that. Let's look at another example. So here's information from equity analysis performs and here we're looking at travel time savings for different population groups compared against the average which is the horizontal black line. And what you can pick out here is zero vehicle and low income households benefit from travel time savings. Nonzero and nonlow households do not benefit so much. This may be due to a focus on transit investments. And the fact that low income and households are more likely to show up in areas that aren't transit rich. -- that are transit rich. This graph also shows female head of households with children and single parent households don't have travel time savings as the average household. That may be of concern and cause the agency to consider a different alternative. This may be because transit investments are serving work trips in those transit rich areas. Whereas the female and single parent households are perhaps not traveling as much to those areas. 

>>> There are a number of them that are relevant to the type of work transportation planners do. Conformity analysis, vehicle missions and energy use are certainly big concerns. Often, MPOs and other agencies are asked to look at general land use growth and many of them are considering smart growth a fact. Growth center concept and that kind of thing. 

>>> So here is an example from Seattle. This is an example and evaluation of energy use by alternatives. Different transportation alternatives. And it was completed as part of long range planning effort. And in this example, fuel and electricity use was estimated from the miles traveled and produced by the forecasting model. And from square footage estimates from buildings land usage forecasting model that was integrated with the system. 

>>> This comparison shows significant reductions in fuel consumption for several planning alternatives and little change in electricity use from the buildings. This evaluation was designed to also account for electricities from electric vehicles. That was actually not considered in these planning alternatives. 

>>> The ones that are five alternatives that are highlighted in light green. And I should point out the light gray one is preferred alternative C. And the dark gray is alternative F. We're going to see another example of that region in a few minutes. Here's another mobility type example from the Sacramento regional model. 

>>> We can track vehicle miles traveled on a per household basis which is a huge advantage over trip base models which you can only track the MP on a link basis. This allows us to understand the source of vehicle of miles traveled which is important. The number of policies are targeted at specific types rather than at network links. Will have lower BMT per household. Models that track it shows it's higher in areas of higher density which is why it is -- so that contradicts what you would get out on an activity based model. That's why it's important to track at the household level. I mentioned Seattle before.  I think we're going to have an example coming up from them. Economic development and quality of life. Are also important. So this is part of a benefit cost analysis. And this shows the same economic prosperity measures to extent which economic development plans produce user benefits. And the cluster on the far left, you see the group of vertical bars labeled as cluster. And those are employment that the plan has identified that show promise, types of employment, type of industries that show promise for economic prosperity and therefore want to promote them. And then another grouping is entitled freight. And that's sectors that serve freight mobility. And another type called high wage employment. There are obvious benefits to attracting high wage employment. And as I showed before there are the 7 alternatives and the light gray and dark gray ones are the most preferred alternatives and seems the dark gray offers most user benefits across all scenarios including the region wide ones on the fourth group on the right. 

>>> So let's talk about nonmotorized transportation which is often an important topic particularly for people interested in health policy and as well as smart growth. Some considerations, activity based models because they are modeling individuals and tend to be specified in ways that include additional spatial detail and again, the nature of the models do have a huge advantage over trip base models in this sense. And there are considerations for modeling both bike and walk trips on the bee hand and supply sides of the modeling system. These include representing pedestrian facilities in the network. Separating bike access from transit. Bike and walk times and accessibility measures along with auto and transit times. Representing walks and bite route groups on the network from parcel to parcel instead of a more simplified straight line. You can also include bike route choices and accessibility measures rather than half mile threshold. Any or all of these considerations could be included in the modeling system to improve the accuracy of walk and bike trips. Fair to mention you could do this to a certain extent especially the network improvements and a trip base modeling system. Where these type of work has been done recently in San Francisco, San Diego, Portland, these are areas are in the process of developing modeling systems. 

>>> So let's now talk about scenario testing. One being reasonableness tests. Whether they make sense. Talk about evaluation of specific policies and projects. To determine is the tool appropriate for the job for which it is needed? And any fine tuning assumptions and specifications. 

>>> Let's jump into an example from the San Francisco model. Scenario testing was used extensively to evaluate parking pricing scenarios in San Francisco. This shows the results of a proposed $3 charge into and out of CORDON area. And this was to be during the weekday peak periods. $3 parking charge in a focused area which you see there highlighted in green. 

>>> The northeast charge reduced city wide more than the focus area parking charge. And of course that was due to the broader coverage of daily trips that were charged. Not just people parking in that area. It was downtown but also passing through. This scenario also increased bike trips much more than the focus trip. What it did not do is increase peak period transit trips much. These parking pricing scenarios would be quite difficult to model using a trip based model. But with an activity based model it's quite possible as you can see. 

>>> In general, when we talk about sensitivity testing, we're talking about sensitivity of model response. Outputs changes to inputs. And one of the things we want to look at is are these shifts, these responses reasonable? So typically we change model inputs using simple factors so the outputs can be interpreted more easily. A change in a bridge toll could be tested by doubling the current price rather than implementing a more specific evaluation. Sensitivity testing can be used to assess the reasonableness of shifts in routes; time of day, destinations, modes of destinations. Value of time segments for vehicle available segments in the population. Something you can do with a disaggregate activity based model. Cannot do quite so well with a trip based model. Reasonableness testing we have to say is much art of science. One needs to consider the magnitude of the shifts. How many people shift time of day. And if all of those things are occurring at the same time, you kind of need to monitor each of these different responses and assess whether they make sense and how do we assess whether they make sense. Are they similar to what we've observed in the past when similar changes occurred? Of course, for new policies especially things like pricing if you live like many of us do in areas that don't have pricing, maybe it's a good idea to look at prices. Where pricing changes have occurred to get a good sense of what to expect. In most cases it comes down to professional judgment and requires a high level of experience with modeling in general. Not necessarily activity base modeling. People have had to do their own reasonableness checks. That's valuable experience as well. 

>>> Here's an interesting example from an Oregon statewide model. This is one where this has activity based travel model but regional economic model and statewide land use model. This particular example, they wanted to test the affects of increases in fuel cost. So they had a reference point and they had costs going up by 4 and 10 times the amount. What they wanted to actually look at in this particular case is how it would affect future land use and development. The model predicted regional center density gaining by 20% in density persons per acre. And other areas zero to 10% gains. If you were in charge of this particular scenario sensitivity testing effort, you'd probably be asking yourself are these results realistic? If gas improves $15 per gallon, is this the change you would expect to see? And how long would it take for the landmark et to make this adjustment? Is the time scale appropriate to which these changes are predicted? And it may be difficult in many cases to answer these questions if we'd not experience such cost. However, activity based modeling systems and in this particular case integrated land use and modeling system would enable you to test the boundaries of that and going through that exercise come up with what seems to be realistic scenario. 

>>> So part of this process one of the things we're interested in is stability across scenarios. And luckily we do have some research to go on. For example, people tend to maintain time budgets in their daily life. So that the total amount of time spent on travel is pretty much the same and increases in one type of travel will result in decreases in another type of travel. So it is useful then to review forecasts of average activity durations to determine if they are stable overtime. The details of various scenarios like vehicle hours of travel or walk access distances to transit can provide insight to stability across scenarios. One of the things we can also look at in dynamic models is are people missing connections? Some of the more advance modeling systems where they've been integrated with DTA and now we're talking about basically research at this point. But very recent research. You can generate daily activity patterns that have decent average rates and calibrated. You may find in simulation they are producing unreasonable patterns where people are missing connections and having to walk home. It's important to look at the outputs and make sure that they pass all the last tests and so forth. 

>>> So another question to ask oneself is is this fool appropriately specified for the job in which it is needed? And of course that depends on the objectives of the analysis. Again, talk about an example such as road pricing options. If weary evaluating road pricing options, we would expect our model to respond appropriately to price signals. So if we increase a toll or a CORDON price or something like that, we would expect people to respond appropriately. And in economic terms we're talking about the elasticity for different levels. If we make a change to -- we impose pricing in our model, we expect the first thing to change or change the most would be people's route choice paths. And after that, time of day is somewhat elastic. We expect to see fairly decent responses in people shifting their time of day perhaps to off peak less congested periods. Intermediate stop insertion and location. Another thing we model in activity based modeling systems. Perhaps not as much as time of day or route choice. We would expect to see some response there. If the pricing scheme frees up the capacity for some segment of our model population, we might expect those people may benefit and insert stops into their schedule. An example of induced demand basically. At the same time if we're adding congestion, otherwise make. Research has also found that things like tour mode are lessee lass tick than perhaps these other route time of day and intermediate stop choices. Perhaps people are more set in their ways in that sense. In terms of location choices, that's quite inelastic when it comes to work location choice. And because we have a disaggregate model and individuals and household types. We would expect the appropriate response from persons and from households. And so for example, we would expect a greater willingness to pay tolls for people with higher incomes and people with trip context such as they are going to work or school or college perhaps. Expect to pay from people who own a trance responder. Already invested in the notion they are willing to pay for tolls to some extent and that's actually one of the reasons why we have those mobility choice models that would predict trance ponder ownership or possession. And we would also expect lower willingness to pay from people from lower income groups and people who are their trip context or tour context is digressionary purpose. They don't have to make it or perhaps choose any location or time of day they want. And for people who are transit pass holders. If you are a transit pass holder and there are highway tolls, you've already demonstrated an interest in taking transit. You are probably less inclined to want to pay tolls on the roadways. Fine tuning assumptions and specifications. For each planning application, it is useful to fine tune assumptions. And the sensitivity tests should be specifically aimed at understanding the model in the context in the specific planning application. Counter intuitive results or a model that does not respond should lead to changes in the assumptions or specifications of the model to correct these results. Changes in input data or assumptions about specifications say for example specific choice model. Maybe it's your time of day choice model. Maybe it's your tour mode choice model or pattern choice model. May need to revisit some of the assumptions. It's useful to consider the confidence we have in forecast variables and whether we need these forecast variables for policy testing. Sensitivity testing can be used to include variables for policy testing we may have left in forecasting. In addition, we might want to look at scenario management and different am turntive few fewers. Cost structures to get a sense of what our model is capable of handling. Can it handle gasoline prices that are for ten times what they are today? What's it do. Does it under respond or over respond? And along what dimension? Route, time of day, mode. Et cetera. One of the things that is particularly interesting is risk analysis. And not everyone has time for that. Let's be realistic. But for agencies that do have time, it's interesting to assign probabilities to different distributions of inputs and run multiple scenarios and see how that changes the outputs. That's something you would want to do with the pricing scenario and that kind of thing. Okay.  If we're talking about performing an alternative analysis there are clear advantages in the ability to summarize outputs by virtually any available household or person attribute. Any geographic stratification and by the different time of day periods that appear in the model. And of course, simulation of outcomes has some theoretical advantages. But also presents practical challenges in handling STOCASTIC affects. It will produce slightly different outcomes each time the model is run. And it's easier to understand and interpret if these facts are minimized to the extent possible. Useful to understand. Forecasting a single number. Distribution of outcomes. Doing away with trip independence assumption. New ways of interpreting outcomes as we mentioned earlier. Taking into account interpersonal linkages between household members as well as tour level versus trip level decisions. 

>>> On this issue of STOCHASTIC variation. It's more realistic.  And you have the ability to portray risk. But of course, there are these challenges that you need to demonstrate that the random variation in the model does not swamp the meaningful changes in policy variables. And as I said a minute ago, nontechnical decision makers may prefer a single number. Some analysis do require analysis of comparative statics. That's also something to keep in mind. 

>>> In the process of achieving a desired level of confidence in the STABLTD of a forecast -- stability of a forecast, there comes this issue of a number of iterations. The desired level of confidence may be for link volumes, mode shares, tour lengths, trip lengths, work destinations and other destinations. Sometimes determined which specifying a level rather than a fixed number of iterations. In many ways this is similar to the feedback loops you do. It really is the same principle at work. You also have some of the micro simulation which means you may need to do things a little differently and in fact, one of the things we do differently is to the extent possible, perhaps more times and try to impose constraints on random numbers being generated. 

>>> This example shows San Francisco county transportation on simulation error. You can probably read the figure label is trips per person. And the % difference from the final mean. They wanted to get a sense of the stability of the system based on the number of runs they've required to reach a stable solution. And as you can see, it gets pretty close pretty fast around 10 or 11 runs. And then around 30 or 40 it's quite stable. And they are looking at it as less than 1% difference from the final mean after 10 runs actually. And you see the county level mean in dark black and there's a neighborhood mean. And individual TAZ mean is the thinner more variable line. And there are a variety of strategies for controlling. Random number sequences by starting from the same random number seed to eliminate the randomness in the activity based model system which you can do to a large extent. Another is to run the model multiple times and average trip tables from these runs. And I guess a third strategy is to hold certain model components fixed between model runs. There are also feedback and convergence issues with the integration of network models which we discussed in previous webinar. 

>>> Okay.  So that concludes the first half of today's webinar. I guess at this point before we turn it over to Peter, we may have time for some questions. 
>> Great. Thank you.  There are just a couple questions that I think are good ones. There was one referring to the travel time difference which for those who might remember slide 18. The question is isn't this information mostly sort of an academic and a research question of how that might be used in practical context of how you've seen it used? 
>> So yeah, that was the one where we looked at the travel time analysis in Chicago and looked at it by different income groups and different person types. And so the question was isn't this information largely of interest into academic researchers? I would say it's of interest in equity analysis. I think that's the context in which this information was intended to show, you know, precisely how different subgroups in the population are affected by a particular scenario or policy or plan and how that varies among income groups or specific types of people in the population. So certainly of interest to academics and researchers but for an agency that has equity issues they need to consider. 
>> Okay. Great.  And one last question then and Peter I'll turn it over to you. And we'll hold the rest of the questions for later. You mentioned that elasticity is different for different decision levels which makes sense. How do you accomplish that if you use mode choice log throughout the model chain? [Inaudible]. 
>> It does. The person asked the question is right that it does have an affect. If there's a change that affects mode choice at that level, it will be reflected that percolate up to the upstream models. The affects become aten waited somewhat. -- excuse me, I'm talking in the wrong direction. As you go down the up -- up model chain they become less. And down the model chain they become greater. 
>> And we can also come back and revisit this question. I think there's another question that was similar to nature to this. So we'll come back I think to that one too. 

>>> All right. Peter, I hear you ready there. I think it makes sense to transition. 
>> I hope so. Let's just double-check if you can hear me right now. 
>> Yes, we can. 
>> Thank you very much, Steven. And thank you very much John for a very good first introductory part which makes my job a little bit easy. So let me also say that I'm very glad to see again a large number of participants. It's great to see so many folks sticking around despite the fact we are going into more technical details. It's becoming more and more demanding. This is great.  And 

>>> In our second part sew far what we normally do, we will go a little more in details and show you some examples as usual. And activity based models is along history which is kind of comparable to the history of four step models not as long than here. Not as long as 30 years. But take into account some of those situation becomes very close in terms of maturity. And specifically we have activity based mobiles in practice like for example, the San Francisco county model. Some of the first models in years. Really have collected a large number of examples. And this is a good chance to recap structural features and specifically contrasting them. But now, not that much in the theoretical context but more in actual application which makes it more interesting for most of the participants. We will talked to about specifically examples of applications of activity based models for the transit studies and new starts. And condition pricing in particular. Conformity analysis and RTPs and EIS studies and so forth. What you can see right now is the screen for most. This is our bread and butter in a sense that's what covers most of the needs. Not all of them but many of the needs with travel demand modelers have to do. So before we even go into any census stuff, what is very important is to see that those are still applicable useful and perform better than the four step models. And that's the major purpose of my presentation. And specifically you see those important instances. I assume again most of the people here have four step models. I will highlight those differences in what activity based models for doing this particular types of study. So far, by the way, there have not been a single reported case of let's say visible failure. But there are been many interesting challenges in specific situations I'd like to address. Overall, we are good in a sense and very positive. 

>>> So let's start with activity based models applied for the new start analysis which is very important direction of great interest. There are a lot of agencies interested in this and the models for this particular purpose. And we have a bunch of interesting application and examples. And basically trying to present them. In San Francisco in Columbus, Ohio and New York where the models were applied for different types of projects. In all those three cases, the model was basically taken through the entire chain and was eventually adopted basically and accepted by FTA. This is proved to be not a simple exercise. At first sight, activity based models might look like an overkill. FTA requirements are pretty much restricted. FTA does not require [Inaudible]. It acquires a certain conservative approach. Don't want to hear so many different. Actually want to see particular concepts. But emphasize the model calibration and the results in terms of telling the story of the project specifically transit markets. If go through the FTA, have to jump through all the hoops. The purpose is prove this project is good and safe. 

>>> FTA requirements start with a six step table concept. Across [Inaudible].  Normally a large scale approach like commuter rail where it didn't exist before. Those tables basically in the four step process may have to be fixed. And [Inaudible] are used. When we deal with more advanced and essentially more flexible activity tool based model, we have to decide what to fix in this structure and how we can calculate user benefits and how this can be compared to the four step process to create a level playing ground and address all FTA requirements. There is a second important issue. The activity based model is the model that produced micro simulation. Than a set of old GPS. And there might be multiple trips to the same. So to squeeze all this into the FTA and make it incompatible that is recommended by the FTA to use the model outputs, we have to do some homework. And we have to basically have net aggregation of individual records. Forced that model. 

>>> Just to remind you, this is a basic way how it's normally done with a four step models. The steps of [Inaudible]. The corresponding assignments and rerun for each scenario. And they use benefits are skimmed off of the -- this is the basic structure how it's done. When it comes to -- when activity based model, you can't replicate all this. This model has more steps. The applications was implemented to the San Francisco county activity based model. You can see that can be calibrated to different levels. And that's how it was done there. And again, a part of the model has to be fixed. But this time, the fixed part corresponds to the basically to a generation to time of day and to primary destination parts. And then this application part has to be again fixed and benefits are calculated. One actually molds are allowed to change within the certain constraints for each. This is not a simple formula. It's more complicated than the four step model. But eventually basically were accepted by FTA in the first cut and that's how normal the results look like. Like certain portion of levels. You can see in this example, made was this model to specifically eliminate double counting of which is not a simple task. Tour and trip are closely intertwined. They are conditional. So certain efforts were made just to separate them. In this particular fitting, total benefits are calculated as some. And majority of benefits by virtue of this calculation. So this first application was not completely consistent. We continued working and specifically in the process of analysis and discussions with FDA is certain interesting issues came up and normally would never -- the issues would never be discussed as a four step model because everything is done at the three level. When you have two and three levels within the same model, you have certain issues. One of them is that actually user benefits based cannot be totalled across conditional choices. -- independent choices. 

>>> That's a big problem we proper trayed portrayed. 

>>> This is all simple because you can just total benefits directly total benefits. But in reality, what can show if choices are constrained, if choices affect each other, for example, in an activity based model structure, decisions are constrained by the total level decisions. They are not independent choices, they are conditional choices. It's wrong mathematically to total user benefits. And under circumstances, the right way to do it is to actually use the upper level benefits only. Assuming the lower level of benefits are selected. Through specific other ways. Through all these conversations and analysis one thing blame clear which might be a big problem for four step modelers presenting the results to FTA. Essentially a four step model has the same problem but it's hidden. It's kind of overlooked. Not home based is highly conditional on the home base step. And you cannot consider independently. In our conversations with FTA, we showed many examples where calculation of user benefits for nonhome base trips. And our recommendation was to drop those benefits. And not to include them at all. Basically, because those benefits are meaning less. And we have shown many results. Those benefits can be turned upside down if you properly apply conditioning. Nonhome based decisions are not independent. And highly constrained. Cannot use any more. If you properly -- FTA currently has not yet decided to use our recommendation. But might happen in the future. So it's a warning for four step modelers we fight four step models. Soon, you will not be able to use nonhome base. Which constrains you and make an activity base model attractive to present benefits. Much more consistent. 

>>> So following this methodology with the Columbus model, user benefits were accumulated at the single level. Assuming that everything else is basically included in the process. And includes all subsequent decisions. Interestingly in the process of the discussion of the structure and demonstrating those benefits to FTA, willing to relax even the FTA requirement.  And actually in this particular application, this was relaxed and here at [Inaudible]. The user benefits are calculated directly. So this way basically we were able to elect the basic requirement that placed it with the requirements that only the primary destination has to be kept equal. 

>>> Interestingly also this is linked to the model structure. And a similar approach was applied for the New York model. Since the whole model has a different structure and particularly a different placement had to do some homework here. The principle was kept the same. User benefits are accumulated. So everything before that [Inaudible]. 

>>> And this way user benefits include all subsequent choices as well. The time here is actually becoming in New York a part of the user benefits. While for the MORPC system. Based on the different model structures. But in any case, the methodology works and accepted by FDA and those differences I -- FTA and the differences just acquire benefits in different ways. 

>>> The second thing I would like to discuss today is issue of aggregation. Specifically for those who are familiar with the FDA process and the summits which are highly recommended. If you want to use the new start findings or have to use this program. Which actually does a very simple thing. This program loops OD pairs. And calculates user benefits and very useful statistics will be scrutinized in your application. And this calculation of the benefits is essentially comparison where it requires more probability 

>>> This can be transferred and becomed individual record. But so far this version hasn't been released yet. So theoretically could be just in activity based modeling version and that would be the end. But since this version hasn't been created, we have to process the micro simulation output which includes individual tour records. And aggregate them in a way that would look exactly like output of four step model structure. And again, one of the challenges is to aggregate different individual records that belong to the pair. This is not a table task. Because it's easy to calculate aggregate probabilities. If I have a set of records, a set of tools with the same origin and destination and I want to calculation probability, this is a simple summation over the records. If I want to calculate the utility, it's actually a [Inaudible] task. We want to calculate utilities in such a way that our aggregate probabilities and user benefits measures would be preserved exactly. Simple solution like taking average over individual utilities belonging to the same pair wouldn't work. Because LOGSUMS and log probabilities are not probabilities. 

>>> Something more intelligent has to be done. Let's show a map just as an illustration. It is showing how you can bring activity based models and force this models. There is still advantages of an activity based model in terms of basically having proper accounting for entire total levels of services. Mechanically the output is brought into a form that can be directly compared to the four step model which makes in this particular case, those comparisons really easy. 

>>> So the activity based model on the left-hand side gives us a full out for each mode I with no probabilities to choose mode. And the corresponding utility for each individual N and each mode I. Aggregation of probabilities basically getting rid of index N. We want to aggregate across all individuals. It's a simple summation. This is very simple to implement. However, calculating utilities averaged cross individuals has to be done accurately. There are two conditions which I show you right now. Slightly more complicated. But essentially can be implemented in the same way. First, we want to ensure our aggregate probabilities are replicated. And those yellow parts correspond to the dependent variables that we want to calculate. So our utilities are going to duplicate our probabilities. The second important condition is we want our LOGSUM to replicate exactly the detailed LOGSUMS for every individual. Fortunately, those two conditions can be combined. And there is a unique solution. We can say that exponents should be directly proportionate to aggregate probabilities. And this scaled C by the second condition that requires the LOGSUM measure would be to the sum of the other individual LOGSUMS. Substituting all these and doing some very simple math, basically obtain a unique solution saying aggregate utilities has to be calculated in the way you can see right now on the slide. And this is a unique mathematically close solution. This way replicates utilities and user benefits exactly. And basically you can substitute very detailed disaggregate models this way with output that looks like output for four step model. But no one four step model would give you those. But you can convert activity based model output into this format. And that's how this interface basically has been implemented. All these calculations are done automatically. 

>>> Technical implementation you have to go through these simple steps. First of all, you need restart version of activity based model which freezes what you don't want to rerun. And run only mode choice and subsequent chain of models and not chain them for the subsequent scenarios. Then we have a post processor that creates a summit input file. And in the process, this bullet implemented the same way if you would do it with a four step model. There's no difference and there was a quality of the results themselves. This is how we can do our basic bread and butter work with activity based model. But eventually just better told to do same type of work. And even make it directly comparable to four step model. 

>>> Another example I would like to show you relates to the several pricing studies. Maybe one of them we will consider. A project that didn't happen but the modeling and planning work was very interesting example and accepted by all clients. It's political reasons I don't want to discuss today. All technical steps were implemented from the very beginning to the very end. It was an example where we needed a model to address many specific practical questions about pricing. The questions were where to price. Symptoms of the area itself. Just use existing bridges and tunnels. Should we use some other system. Should we allow free bridges and tunnels. A lot of questions related also to the tolled scenarios and schedules. Should we start pricing at 6:00 in the morning, 7:00 a.m.? 8:00 a.m.? Should we stop it by 8:00 p.m. or 10:00 p.m.? Should it be in both directions or only in the peak direction? So many practical questions you have to answer. Eventually resulted in hundreds of different alternatives. 

>>> And some of the equations were specific and certain pricing options I myself learned for the first time that he didn't know about options like this that we will be discussing today. In the process much application for this analysis, we found many beneficial features in a total level context. Makes it much more logical to think about tours rather than trips. You cannot answer a simple question how people actually enter the area and leave this area. Actual toll schedule depends on the boss. 

>>> Also you need a complete model in this case. This is not an FTA. This is actually type of a study where practitioners want to see wide range of affects. And this specifically want to see elasticity with respect. Pricing should change.  And we want to understand to what extend pricing was successful in shifting people to transit and what extent it was not. But also it might be time of day affect which were also easy to understand. As well as radical pricing strategies like -- something between 8 and 14 doors per day. Those types of radical pricing models will change as well. It was actually expected many people who changed their destinations or maybe even cancel or implement some other would be encouraged to do it in a different way. Here, we want to be able to portray those affects. 

>>> One important thing John also mentioned in his presentation is what saves us and gives this opportunity to answer these questions [Inaudible]. Basically in every individual and analyze features that this individual made between the base and the scenario and summarize them in different ways. And separating them. 

>>> But still, even with the sophisticated activity based models, there are tricky cases. And one of them I would like to show you specifically demonstrating the power of micro simulation technique. Basically license plate rationing. But license plates rationing is an example of efficient strategy that doesn't directly involve tolling but involve constraining using of autos for certain destinations. 

>>> License plate rationing is a simple idea where different cars are banned from entering a certain pricing area or using certain toll facilities. For some days, for example, some vehicles based on license late number, last digit are some of the principles. Every person knows this vehicle was one particular day cannot be used. And this particular case I showed you before it was example of 20% rationing. May be 10% or any other stronger or bigger policies. 

>>> And then an interesting question we apply a strategy like this. Some people basically losing temporarily randomly through the week. They are losing their ability. So how would that affect different choices? How could we even approach modeling? This is not just a time and course issue. This is a car availability issue. Car availability fluctuates from day-to-day. So in aggregate model we cannot address an issue like this. If you have a four step model, would be sorry I can't model this. Let's do something in Excel maybe. But I don't see a direct way to do it. With an activity based model, we found a way to implement it through the specific chain that created the specific version of this model. I won't go into details but this model certain things were fixed. The basis was the New York activity based model. Didn't reduce this model but have to adjust a little bit. And you can see here, this interesting component of the model that's responsible of auto sufficiency calculation based on the car ownership. And this component was allowed to randomly fluctuate in the model application on these license plates. So basically what happens in the process of micro simulation and this is a pure advantage of this technology versus any aggregate model where hardly be able to implement. In the micro simulation, we have a list of households. And in this particular example we have 6 households. Every household has a number of workers. And a certain number of cars. Like the first household has more cars than workers. The car sufficient is 1. And in this particular case people with my car sufficiency. But now what happens here and we today have a little bit strain in the presentation. What happens here is I will use words to describe. For each particular car there is a slot and this household there are three cars and three slots. And for each slot, there is a random [Inaudible]. So some of those cars may become unavailable for certain types of trips. So if we have a household of this type. There's two workers and one of the workers has a workplace in the CBD in the pricing area, for this person, if this car was banned, the probability of driving alone and probability of carpooling would go down. And those probabilities are taken from the mode choice model which is sensitive to car sufficiency in this particular case. So we can say [Inaudible]. Has to find the different solution. So can't really drive alone. He might still carpool but probability would go down. Whereas the second person can make the same decision. She doesn't have to drive to the CBD area. This kind of separation of people is again impossible four step model. 

>>> Working on a large set of different pricing projects brought up yet another very important issue that I would like to share with you. And again, contrast to a little bit four step models and activity based models in this particular regard. Doing a good job in accounting for tolls in both directions? I've been many times and probably many of you as well you have to model different pricing strategies and it's relatively easy to model that I applied in both directions in the same breech or ability. The rest of the model performs the same way. You make it a little bit longer. That's it.  And those are not depending on anything else. So you can do this. However, in reality, at least in my practice in many cases, I had to answer more specific questions. And those questions included those were did I feel by time of the choice. By time of the condition pricing. Sometimes even sitting in the dynamic way. And one of the interesting questions that recently I had to answer here is New York and it was sometimes surprising how nature [Inaudible] unprepared how we are unprepared to answer those questions. Wants to compare one directional toll strategy to two directional poll strategies. And again, sounds very logical because, for example , in New York, we do have free and toll bridges. And they sometimes have equivalent in both directions. So drivers in cars have to slow down twice going to the destination and going back. Assuming most are phonetic. So people have to slow down twice. And you have to pay one toll and then the same toll or slightly different in opposite direction. And the question was why we should do it twice? If we know people go back and forth. Instead of charging $6 in one direction and $6 in another direction why can't we charge in one direction and collect the same amount of money but make the level of service better. And we can also decide which direction is better to use for pricing and maybe make it better for the most important peak hour direction. And eventually gets on and on into more sophisticated. But both directions tolled but in a different way. In this realistic example. One of the examples had to do with pricing specifically. It looked -- the idea was to have a higher toll around $6 in the peak direction in the morning. And have a smaller significantly smaller only $2 for the not peak direction. And to reverse it in the opposite direction but not exactly symmetrically. At peak pattern in the opposite direction it was not equal to the peak pattern in this direction. So the strategies that we had to model are different. $3 in off peak and $5 in the peak period. 

>>> Let's say this is a scenario. How technically can implement this time of simulation? That's what people see in reality. Through tolls they have to pay are highly differential and in this particular setting, we can say there are 15 different types of commuters or traveling assuming that everyone has to go back and forth during the day. Might be even more complicated. But let's assume. We have 15 types of users. Some of them have to pay as much as $11. Some of them have to pay $6. Some pay $9. It's a big deal for the majority of people. If you have to pay $11, you might consider transit. And this would be very strong in the future. Since it cannot be calculated based on one directional toll, you have to take into account both directions. Interestingly enough, everyone understands this in practice. And operate with those types of scenarios and expect us to even tell them which is optimal strategy. But think about actual modeling. You find that with a four step model, it's literally impossible to ensure any reasonable level of consistency. This model simply doesn't feed those tolls. With a total base model still have to make a better effort. But at least a much better can be done. This is a very important practical advantage. Expect a large scale continuation pricing or any program for your region. You would have hard time to answer all these question as a four step. And now the important aspect that everyone has to go through and part of a -- probably like a major task for most of the end polls who did not run specific projects. All activity based models have been applied for conformity successfully. Starting with the San Francisco model and specifically was the subject to conformity. It was primarily created for conformity analysis. 

>>> We have not found any particular problem and you are familiar with the requirements for conformity primarily have to be realistic in terms of application which used to calculate. However, there's certain particular aspects which make an activity based model much more attractive for this particular part of work. It's specifically if this conformity analysis is compliant with scenarios of travel demand management. And specifically also expect some shifts in the month patterns. 

>>> Here are the basic advantage of the act TIVEN it based model or having a better resolution strongly comes into play in a sense that we cannot address the scenarios. And actually, we control certain strategies can improve and actually, you can meet the conformity requirements better. The four step model might portray everything in much worse case that actually we could be. Activity based model in this context is a tool to show that the environmental situation might be improved by travel demand. And this will show you much better than the four step model. 

>>> And if you remember the story of [Inaudible].  We want to better calculate emissions.  And the first order emissions can be calculated based on the VMT/VHT. Activity based models and particular combination of activity based model. Much better way to do it. You can address a lot of individual details and also different types. One of the important improvements. And again, in the context of integrating them with a dynamic is the model type which is very important for calculating emissions. And very important for portraying this pricing. Much more [Inaudible].  In my experience, I expect a specific improvement in the next three years in this direction. It would help us in many cases overcome a certain guidance and show what can be done to improve. Especially for regions where it's really an issue like in New York. And in New York we do have proven statistically cases of asthma for children and other cases ten times more frequent than normally observed. It's very important for us to properly understand and model and recommend strategies that would basically reduce. Many of those strategies get lost. You can't really prove anything. With this type of tool, we hope we'll be able to do that. 

>>> RTP, this is bread and butter. Something every agency has to do. Can be done with a four step model. Also offered some additional advantages. We talked about pricing strategies. There's a bunch of other projects that might be a part of your RTP plan that cannot done better. We don't have time to go into details but include everything that relates to a nonauthorized travel improvementimprovement. Any investment in -- just defies this investment and that's difficult to do with four step model. Travel demand management strategies talked about that. And specifically in the IBM models. Opens the way to proper portray any travel demand. And now having all this experience is difficult to imagine how would you do as a four step model. Probably a way always to do it. Probably again, many good benefits, important to promote those policies would be lost. Basically allows to show much wider range of mappings. That can be used to portray and defy certain investments. You want to be able to prove it quantitatively. And it includes equity analysis. 

>>> Eventually everything is done at the individual level. So you can summarize winners, losers the way you want. And you don't even have to define it from the very beginning. You can see side it in the process of analysis what type of dimensioned you want to analyze. Literally for the individual households by zone or certain types. Energy use, user benefits but now in the context of economic development. And also all types of travel. And it might involve basically a lot of different individuals. Destination which is frequency features and with this type of analysis, it's easy to separate them and to present them in any kind of interest summary and show how any kind of change in the regional transportation network would use or suppress traffic or travel. 

>>> Now, a little about model performance. Since it sounds a little bit scary, millions of people simulate it literally. Would it take so long and it's known that the first version of activity based models just because of the limited computer power. The idea of micro simulation [Inaudible] in early 80s literally more than 30 years ago. But this power and ability efficiently, that's what's important. That's what happened. Basically only recently. When we talk about model designs. And we would like to bring more and more submodels and more and more features into this model. There is a down side of this model performance. And specifically which we have to balance when you decide about model design. Some people light say computer power is limited. There are many components of models we would like to zone. With all developing of hardware and software, we're still limited. A couple examples everyone would understand. We would like to switch to a better level of special resolution. For example, instead of 3 or 4,000 zones, we would like to use a select number of zone. Or even malls of parcels. However, building a map at this level is still feasible. It's still today to operate at the level of service matters. Have to apply certain designs to take advantage of the levels of resolution. And another good example of something you have to do is getting the change and how implement in activity based models. Would have been simple to implement in a very consistent way if it had been possible to integrate. At three dimensional, 4,000 by 4,000 by 4,000. Not talking about 4 or 5. Rises to overcome this computer of limitation. Now hardware and software considerations also a tradeoff. It's getting better and we have a new generation. Those constraints have to be taken into account. There are more and more tradeoffs basically in a sense that by just paying a little bit more money for your hardware, you can actually improve your fire power and overcome any of those problems. 

>>> In terms of the so far ware implementation, most of the activity based models are custom application programs. Because of the models cannot be squeezed. And then implement it using scripts which are based on calculations. Here we need more flexibility. Cannot be implemented using standard or any of something else [Inaudible]. To create some facilities which would allow you to -- it's still in infancy yet. There are two examples of I would say make sure custom consultant develop software application. Developed by BOWMAN and Bradley. There's multiple applications and as you can understand, when you have so many applications, you really have to think about design of the software that you can't really just develop in a nice way writing the code and next time taking the code and ejecting it. You have to do something more Intel JNT.  The difference between just a single mode code and a platform is the platform is a package that allows you to construct an instant of model. You need this level of flexibility. The core procedures are basically the same and it allows you to create important generic parts of the software that over the years becomes pretty much. There are many more software platforms designed. But many of them represents interesting things. 

>>> Of course there is a set of user productivity issues. I wish we had more time. But [Inaudible] we go careful. This whole list explaining how this model is going to be implement ed and run. -- implemented and run. Actually have different requirements primarily based on proficiency and desire to be able to manage all this by themselves. Some clients they take a passive user positions. They would like to have very developed GUI and scenario manager, something you have to push on several buttons and everything else would be taken care of. Some of the clients prefer to have it open and to do things in a certain way basically that makes them more involved in this process. Very important and makes it user prod you cantive. I could say that activity based models with all the complexity that they have been brought to the same level of maturity as a four step model. There are data structures and computational requirements. I will go briefly because we only have several minutes left. Again, it's kind of a design issue and the latest tendency is to develop a parallel database. Makes data more convenient. Rather than just set of input and output text files. 

>>> Can talk about run times there are several interesting studies. Some are actually out dated. We're talking about something five or six years ago. But can show a certain history. The bottom line, you can see the run time is inversely proportional to the fire power of the clusters. By spending certain,000 of dollars more -- certain thousands of dollars more, you can scale it down to minutes and hours. It's a tradeoff. 

>>> We have dimensions with moving the dimension of fine grain spatial resolution. Which also results in some compromises that's a certain price to pay for it. Easy to do but a lot of advantages using micro zones. If you have in mind -- find a level of special resolution. We have ways to resolve all computation ratios. 

>>> There are several interesting research areas we have to probably discuss. Including where we'd go with all of this. Again, there is nothing as beneficial for development as real application. When we play with models, you don't see the advantages and disadvantages. When you apply them for your studies, then you learn sometimes hard way. Most frequent experience was has been positive. How we can really handle better or take advantage of variation. Specifically in the context of example for uncertainly and travel time under liability which is something which we encounter in reality. Now we're thinking about suppressing all of this and how we can take advantage of all of this and make it a part of the model like travel time and liability. And of course how we can make it faster using basically better and better computing power. We have done a lot of work with respect to the variation. And this is an interesting result which comes from ALBATROSS model from Europe. Showing it's extensive set of examples. It's an example of a model with higher level of variability. This model models different days of the week and how people schedule their activity. In this model one day can be different from the other day. Like in reality. And it was shown that certain number of runs, we can squeeze it under but sometimes becomes impractical if we want to have it at the level of almost fix. 

>>> Data visualization has become important direction. 

>>> Example of consorted example which is a tool to an activity based model which creates wide range of outputs and gives an option to manage outputs efficiency. For the particular model structure of course. Many tapes of analysis.  John showed you over here more chairs by person type. You want to know what are the people using particular models. 

>>> And this analysis gives those FTA type answers to the questions. Who are the people primarily benefit. It allows you to analyze some dimensions but distribution of people by age specific affects. It can give you some cases for creation and other types of analysis. What people do during the day and where they are located by hour or one minute resolution throughout the day. 

>>> And finally, for example, you can specifically this specific graph is -- you can see who are people suffering from condition. And in this particular compilation, what you can see is not just travel time but this is for each type of person through the entire chain which is calculated through congested travel time versus free flow travel time for auto users. You can see like interesting affects we don't think about and you never can have something of the sort with a four step model, of course. Where you can see highly differential. Certain types of people and you can find them also geographically or whatever. Or by age. Suffer much more than other people that suffer less. 

>>> This is another example of one taken from the day SIM application. You can track everything and see the entire daily pattern for a certain person including all travel times and activity locations. So for example, you can demonstrate -- take individuals and analyze them at the most possible disaggregate level. And see the behavior during the day and before and after. 

>>> High performance computing is related here. And for example, we frequently say that travel more today in activity based model has to be interdisciplinary in a sense you have to be a little bit statistician. A little bit of transportation planner. Understand the reality and a little bit of computer science. Conveniently squeeze those and make it efficient. And specifically, multi threading and using computer. Has become a norm in activity based model. 

>>> So running a model on the [Inaudible] is kind of different from just typing codes. It requires professional. If you do it professionally, can take huge advantage of this technology and just by making distributing properly, you can speed up the entire model performance by a factor of 10. This is very important. There are new options. Constantly climbing from the high tech technology. Maybe we'll see something else in terms of how the models are implemented. But again, to catch up and be on the wave, we need all activity based modeling [Inaudible] have professionals that do it. 

>>> Now, just a short summary.  And I think we are about maybe would like to leave time for questions and discussion. Basically, I hope through the webinars and this one which wraps us all up, we talked about that the main conceptual advantage of doing all this is disaggregated of representation of individual behavior. Wealth of useful information that you can use for this types of analysis. And you don't even have to decide in advantage what are the dimensions that you want to analyze. Like in the for step models. If you don't have them you never have any. Here you don't have to do it in advance. You can always decide it in the process of analysis. What type of additional information you would like to take a look. Because, essentially it gives a full set of activities for each person. A full set of individual. This is probably the lowest level of details you can have. And Effective way in decision making. It presents challenges. Sometimes you have too many. Sometimes it's puzzling. Millions of records. Hundreds of fields. Eventually want to make a decision and present it properly to the policy makers and as a Stakeholders, specifically who used to see very aggregate very standard outputs. And of course you can replicate everything you can do. Like in the FTA type analysis, you can basically teach this new dog old tricks. But this is not our final purpose. It's take full advantage of this new information. But we have to do it in a smart way not to over welcome people. No one is going to -- it's important to understand top level policy makers. Meaningful summaries.  It's kind of an issue we are working on how we can create useful. Visual clear reports basically. Supporting any type of decision. 

>>> So enhanced data visualization is important to take advantage of this rich data. Also basically over the years started moving towards some kind of standards clear to nontechnical people and to create certain standards and basically were successful to create those and basically make it a keen process from technical people to take activity based model summarize the results and being able to make some concludes. -- conclusions. 

>>> It's of great promise.  Activity based model by virtue of the simulation, 99% implemented with active based models, they land themselves [Inaudible]. Creating a structure which is relatively easily. Models can be separated by household persons or zones. And in this particular case becomes inversely proportional. And conceivably on this computer [Inaudible]. It's literally a simple tradeoff. 1,000 CPUs doing all this and reduce it two minutes. We don't have super computers like this. But we can configure a reasonable cluster. That's what we do today for most of the clients. With all SOIS if I indications -- sophistications. Now, it's back to you Steven. Thank you very much for your attention. I think we have some time for questions. And I'll be glad to continue this discussion with you. 
>> Great. Okay.  Thank you very much. That was great.  And we have a couple questions.  I did want to remind folks that what you see on the screen is a survey that we want everyone to fill out. Shouldn't take you long to do. And do so while we're asking and answering questions. 

>>> First one Peter is for you. It speaks to new starts again. Have you found difference between user benefits calculated from the for step model and ABM model in the same area and year? 
>> Well, it's a very interesting question. I'll be honest with you, we've never really implemented completely on this exercise. Running a four step model and activity based model for the same project. I don't really have an exact table showing sides to sides. I have something that came up strongly when we discussed the results. One of them is that an activity based model does a much better job which actually sometimes work in favor of the project. Sometimes might show differences to address the spin of the services. When you consider alternatives when it improve frequency for a wide range of hours. The four step model tends to kind of overcome all this and create a very small additional benefit. In the four step model you have 80% falling into the peak period. Any improvement is not really proper. When you apply a four step model, normally you would favor in terms of benefits improvement of services in the peak period. And benefits very look like not enough to defy any improvement. Only 60% of commuters commute both legs in a.m. and p.m. period. You have around 40% of commuters that actually have at least one leg and for many of them, lower frequency service or no service is factors that divers them from using transit. For example, if you conceivably don't have a hand in direct proof. I do expect that an activity based model would distribute benefits between peak and off peak period to a four step model. For some types of project it would more realistically shows improvement of services companion to improvement in the peak period is important. And this is overlooked by four step model. 
>> Great. Okay. There's a different question. Has to do with how we handle grouping of trips relative to space. Can an ABM system be executed or designed in such a way it accounts for consolidation of activities. And the example they give is going to shopping mall or shopping center to consolidate multiple shopping trips? 
>> Would you like me to take it? 
>> Yeah. Why don't you start with that and then John can jump in with anything further. 
>> Yeah. Okay.  There are two particular aspects where with what you are saying consolidation of activities or bunching them or bundling is very important. We do have it in models. One aspect is consolidation. Shopping malls are examples where people frequently go for one period of time and frequently entire families and combine shopping activities. Those visits as separate segment. It's not just shopping. It's a combined type of activity. It's one direction. Second one by taking advantage of finer level of special resolution, we can portray accessibilities. We have a set of exit zones. We can better analyze accessibilities and create more exact change including the conditions. A shopping mall eventually might be modeled not as a single TZ or MAZ but separate individual locations connected by pedestrian links or whatever. And any improvement in terms of accessibilities making, for example, shopping malls walkable or things like that. Would also affect the choice of those activities. 
>> DOUFSH anything? -- do you have anything? 
>> It was a very complete answeranswer. You are typically choosing primarily purpose and destination tour and if it happens in your model system, you choose a zone or parcel that happens to be located at shopping center, chances are the accessibility to other shopping opportunities and other types of activity opportunities is going to be very high in that location. As Peter was detailing, has a lot to do with the spatial resolution. As a result of those accessibility opportunities. You may start off with initial primary trip and purpose for the tour, but because of those accessibility opportunities, you're also likely to generate one or additional trips and so that's one way it would evolve in the output from looking like the cluster of trips around that particular area. 

>>> There's one last question. Peter, I know you are doing work around reliability. Has anyone used the results of ABM for travel time reliability analysis? 
>> Yes. We have gone through this a very good question. I'm glad you ask it. It so far has not been a part of your routine for what we do but we have developed these methods and reported them which is like a research project. Travel time and liability is a big deal. And it's very important factor. And the new generation will include it. The fact that activity based model has this reliability which before that had been considered a problem, something we would like to suppress, actually came very handy and one of the methods that we applied and already put in practice at any moment of time is to use an activity based model to generate a series scenario type tables rather than one. And see distributions of travel time and calculate standard deviations. We have a complete version of the New York model having this feature in place. It's still a little bit unusual. Yes, we have made progress and know how to include reliability and take advantage of the activity based model. 
>> Peter, you are [Inaudible]. 
>> Yes, it's a combination. More specifically, there's a complete project that has it as a component. [Inaudible] takes it one step further and applies subsequently dynamic traffic assignment as traffic analysis too. Which uses those scenarios generated by activity base. 
>> Great. Okay.  Okay.  Well , that was the final question. And so with that, I want to thank everyone for your attention and interest in this topic. And willingness to participate in it. Thanks for all the great questions you've asked over the course of 12 webinars. We've enjoyed this.  I hope it's been useful.  Just a quick reminder we have recorded all of these and as both John and Peter mentioned, they are available on the web site. You can access those and download the presentations from those as well along with the speak notes. 

>>> So thank you Peter and John for your presentation and thank you everyone for your participation. 
>> Thank you. 
>> [Event Concluded]